# Scheduler 当前架构与 Phase 1 拆分优化说明（决策版）

> 文档版本：v1.0  
> 更新日期：2025-12-18  
> 适用对象：决策部门 / 基础设施 / 后端架构评审  
> 目的：概述当前调度服务器（Scheduler）架构现状、存在的扩展风险，以及已落地的 Phase 1 关键拆分优化（保证单机可运行、为未来 cluster 改造预留空间）。

---

## 1. 一页结论（Executive Summary）

### 1.1 当前架构是否可用

- **可用**：当前 Scheduler 在单机规模下可稳定运行，具备完整的节点注册、心跳、节点选择、会话处理、任务下发与结果回传能力。
- **风险可控但可预见**：当并发与节点数量上升时，瓶颈主要来自“调度主路径与统计/外部 IO 耦合”以及“全量遍历 + 锁竞争”。

### 1.2 本次优化做了什么

本次完成 **Phase 1 的核心一部分**：把非调度主路径的高成本模块拆出并缓存化（单机照跑，未来可替换为 Redis/独立聚合器）。

- **把 Dashboard/统计从同步请求里拆出来**：`/api/v1/stats` 改为**只读后台生成的快照**，避免每次请求现场遍历大量状态。
- **把 ModelHub 服务包列表从统计逻辑里拆出来**：服务目录改为后台定期刷新缓存，统计生成时只读缓存，避免控制面路径出现外部网络 IO。
- **为未来 cluster 预留接口位**：新增“服务目录缓存组件 / 统计快照组件”，未来可将实现替换为 Redis/独立聚合器，而无需改动 API handler 和调度主路径。

### 1.3 直接收益

- **控制面更稳定**：移除了 `/api/v1/stats` 请求路径上的外部 HTTP 调用与重计算，避免“Dashboard 访问/轮询”反向压垮调度进程。
- **更符合可扩展架构原则**：统计与目录服务从“同步强耦合”变成“后台刷新 + 读快照”，为后续多实例/cluster 做准备。
- **单机不受影响**：所有改造均为“内部解耦 + 缓存化”，不引入 Redis 等新外部依赖；当前单机部署方式不变。

---

## 2. 当前 Scheduler 架构概览（现状）

### 2.1 角色定位

Scheduler 当前同时承担：

- **会话入口（WebSocket Session）**：接收 Web 客户端音频片段/Utterance，累积音频并创建 Job。
- **节点入口（WebSocket Node）**：处理节点注册、心跳、任务结果回传、错误上报。
- **调度与派发**：根据节点能力与资源状况选节点，并把 Job 下发到节点执行。
- **运行期统计与 Dashboard 支撑**：收集活跃会话、语言使用、节点/模型/服务包统计等信息，提供给前端页面/接口。

> 说明：当前实现仍包含“音频 payload 处理与缓存”，因此 Scheduler **并非严格意义的纯控制面**（这属于后续更大规模演进可选项）。

### 2.2 核心组件（代码模块视角）

- **节点注册表**：内存 `NodeRegistry`，`RwLock<HashMap<node_id, Node>>` 保存节点状态、能力、资源使用率等。
- **调度器/派发器**：`JobDispatcher` 创建 Job、选择节点（当前为遍历候选节点并按最少任务数排序）。
- **会话与连接管理**：会话管理、session/node 连接管理、结果队列、音频缓冲等。
- **统计模块**：`DashboardStats` 聚合会话与节点信息，支撑 `/api/v1/stats` 与 Dashboard 展示。

### 2.3 扩展瓶颈（为什么需要 Phase 1/2）

主要风险来自以下三类：

- **同步耦合风险**：统计与外部服务（ModelHub HTTP）被放在同一进程、甚至可能在请求路径上执行，导致控制面抖动。
- **锁与遍历成本**：内存注册表受 `RwLock` 保护，节点选择需要遍历与排序，规模增大时放大 CPU 与锁竞争。
- **风暴类事件放大**：如模型缺失/版本切换引发 `MODEL_NOT_AVAILABLE` 风暴时，若在主路径做昂贵操作会放大故障。

---

## 3. 本次已落地的 Phase 1 拆分优化（单机可运行）

### 3.1 优化目标

- **调度主路径更短、更可预测**：减少同步 IO、减少大计算、减少与统计/目录服务的耦合。
- **保持单机运行**：不强依赖 Redis/队列等外部组件。
- **为 cluster 预留空间**：新增“可替换组件”，后续可以用 Redis/聚合器实现同样的接口。

### 3.2 优化项 A：DashboardStats 快照化（/api/v1/stats 只读快照）

#### 问题（优化前）

`/api/v1/stats` 请求时会触发 `DashboardStats::collect()`：遍历会话与节点状态，并可能引发额外开销（例如外部 HTTP 获取服务目录）。

#### 改造（优化后）

- 新增 **Dashboard 统计快照缓存组件**：`DashboardSnapshotCache`
- 后台任务按固定周期（默认 5 秒）生成一次 stats JSON 快照
- `/api/v1/stats` 请求只读快照；冷启动时若快照尚未生成，则兜底现场生成一次（但不再做网络 IO）

#### 收益

- Dashboard 轮询不会再把 CPU/锁竞争推到峰值
- 控制面请求抖动显著下降，系统更“抗误用”（例如频繁刷新 Dashboard）

### 3.3 优化项 B：ModelHub 服务目录缓存化（从统计主逻辑移除网络 IO）

#### 问题（优化前）

统计模块内包含同步 HTTP 拉取服务包列表的逻辑：当 ModelHub 不可用/抖动时，会导致 stats 请求路径不稳定，甚至反向影响 Scheduler。

#### 改造（优化后）

- 新增 **服务目录缓存组件**：`ServiceCatalogCache`
- 后台定期刷新（默认 30 秒一次，带超时保护）
- 统计快照生成时只读取缓存（无网络 IO）

#### 收益

- ModelHub 不稳定时，Scheduler 仍可稳定提供调度能力与基础 stats（服务目录可能短暂滞后或为空，但不会拖垮主流程）

### 3.4 可替换接口位（为未来 cluster 改造预留）

本次新增两个“可替换组件”，后续 cluster 化可替换其实现：

- `ServiceCatalogCache`：单机为进程内缓存；未来可改为 Redis key / 独立 catalog 服务。
- `DashboardSnapshotCache`：单机为进程内快照；未来可改为 Redis snapshot / 独立聚合器写入。

> 这两块都是“非调度主路径组件”，因此替换成本低、风险可控。

---

## 4. 与未来 Cluster 的衔接建议（决策部门关注点）

### 4.1 Redis 引入顺序建议

- **短期（单机）**：保持无外部依赖运行；若需要外置状态，优先用**单实例 Redis（主从/哨兵）**。
- **中期（cluster）**：当确需 Redis 自身横向扩展时，再引入 **Redis Cluster** 与 key 重构/迁移工具链。

### 4.2 Redis Cluster 的硬约束（必须在方案层提前确认）

- Lua/事务原子更新中：**同一脚本访问的所有 key 必须在同一个 hash slot**。
- 因此 session bind 与 node jobs 计数这类“必须同脚本原子完成”的 key，必须用 hash tag（`{...}`）设计。

> 本约束已补充写入方案文档《Scheduler_扩展与容量规划说明（含 Redis 设计）》。

---

## 5. 当前仍保留的技术债/风险（透明披露）

- **Scheduler 仍承载音频 payload（数据面）**：目前会话入口会累积音频并把任务下发到节点。若未来追求更高规模与更强隔离，需进一步将“音频传输/缓存”下沉到更适合的数据面组件（例如 WebRTC/媒体网关/对象存储引用）。
- **节点选择仍是全量遍历 + 排序**：当节点规模显著增大，需要进一步引入候选集索引/快照缓存/分片等策略（Phase 2/3）。

---

## 6. 交付物清单（本次变更）

### 6.1 新增模块（可替换组件）

- `central_server/scheduler/src/service_catalog.rs`
- `central_server/scheduler/src/dashboard_snapshot.rs`

### 6.2 行为变化

- `/api/v1/stats` 改为读快照（后台刷新），请求路径不再同步做“重计算 + 外部 HTTP”。

### 6.3 验证结果

- `cargo test` 全量通过（含 stage 测试）。

---

## 7. 下一步建议（可选 Roadmap）

- **Phase 1 继续项**：将 `MODEL_NOT_AVAILABLE` 等风暴处理进一步去抖/限流，并确保“昂贵操作”不在主路径执行。
- **Phase 2 预研**：引入外置状态（Redis 单机）+ 幂等 request_id + 原子绑定/计数（Lua），并在方案层提前完成 Cluster key hash-tag 设计。


