# 调度器「一直在重新分配任务」与「Job does not exist」排查说明

**日期**: 2026-01  
**适用**: central_server/scheduler

---

## 1. 「一直在重新分配任务」的澄清

### 1.1 调度器实际行为

- **真正的“重新分配”（failover 重派）** 只会在 **已派发任务超时** 后触发：
  - 条件：Job 已成功下发到节点，且从 `dispatched_at_ms` 起超过 `job_timeout_seconds`（默认 60 秒）未收到完成/失败结果。
  - 行为：先对原节点 best-effort 发送 `job_cancel`，再选新节点重派（最多 `failover_max_attempts` 次）。
  - 日志关键字：`Job dispatched 超时，尝试 cancel + failover`、`Job failover 重派成功下发`。

- **若日志里没有上述两条**，则调度器 **没有** 在做“一直在重新分配任务”。  
  你看到的很可能是：
  - **每个 utterance 一个 Job 的连续派发**：多条「派发到节点」日志对应的是不同 `job_id` / `utterance_index`，这是正常的一轮一轮派发，不是“对同一任务反复重派”。

### 1.2 如何确认是否真的在重派

在调度器日志（例如 `central_server/scheduler/logs/scheduler.log`）中搜索：

```text
failover
重派
Job dispatched 超时
```

- **若没有任何匹配**：说明没有发生超时重派，可排除“一直在重新分配”的误解。
- **若有大量 `Job dispatched 超时，尝试 cancel + failover`**：说明确实有很多 Job 在节点上执行超过 `job_timeout_seconds` 未返回，被判定超时并触发重派；此时需要从 **节点处理变慢** 或 **job_timeout_seconds 过短** 两方面排查。

---

## 2. 「Received JobResult but Job does not exist」说明

### 2.1 含义

节点把 `JobResult` 送回调度器时，调度器用 `job_id` 去 Redis 查 Job（`get_job`），若查不到（返回 `None`），就会打这条 WARN：

```text
Received JobResult but Job does not exist, will still add to result queue for utterance_index continuity
```

即：**结果到达时，调度端已没有该 Job 的记录**。

### 2.2 常见原因

1. **已完成 Job 被定期清理**
   - 调度器有后台任务：每隔 `job_cleanup_interval_seconds`（默认 60 秒）跑一次 `cleanup_completed_jobs(300)`，会 **删除“完成时间”距“当前”超过 5 分钟** 的 Job（按 `created_at` 算年龄）。
   - 若节点 **重复发送** 同一 Job 的 `JobResult`，或 **延迟很久**（例如第一次结果已处理并标记完成，5 分钟后该 Job 被清理，此时又收到同 job_id 的第二次结果），就会出现“Job does not exist”。

2. **Job 从未写入或 key 不一致**
   - 若 Job 创建/写入 Redis 异常，或读写的 key 不一致，也会出现查不到 Job 的情况（相对少见，需结合创建与派发日志排查）。

### 2.3 对业务的影响

- 当前实现：即使 `get_job` 为空，调度器 **仍会把该 JobResult 按 utterance_index 加入 result queue**，保证会话顺序和连续性。
- 因此：**仅出现“Job does not exist”不会直接导致“丢结果”**，只是无法再对该 Job 做“释放 slot、更新状态”等操作（通常该 Job 已被清理或已处理过）。

若同时出现 **结果丢失或顺序错乱**，需要再查 result queue 消费、会话状态和节点是否重复/乱序上报。

---

## 3. 建议排查步骤

1. **确认是否真的在“重新分配”**
   - 在调度器日志中搜：`failover`、`重派`、`Job dispatched 超时`。
   - 无匹配 → 没有超时重派，可把“一直在重新分配”理解为“多 Job 连续派发”的正常现象。
   - 有匹配 → 确认为超时重派，再查 `job_timeout_seconds`、节点负载与 ASR 耗时。

2. **看“Job does not exist”是否伴随重复/延迟结果**
   - 若同一 `job_id` 的 JobResult 只应出现一次，却出现多次，或节点在 Job 完成很久后才上报，则容易在清理后收到结果，从而触发该 WARN。
   - 可检查节点端是否对同一 Job 重复发送 JobResult，或网络/队列导致严重延迟。

3. **可选配置**
   - `config.toml` 中：
     - `job_timeout_seconds`：当前 60 秒；若 ASR 经常超过 60 秒才返回，可适当调大，减少“误判超时→重派”。
     - `job_cleanup_interval_seconds`：已完成 Job 的清理间隔（默认 60 秒）；清理保留时间为 300 秒（代码写死），一般无需改。

---

## 4. 相关代码位置（便于对照日志）

| 现象 | 位置 |
|------|------|
| Job 超时与 failover 重派 | `central_server/scheduler/src/timeout/job_timeout.rs` |
| 重派原子更新 | `central_server/scheduler/src/core/dispatcher/job_management.rs`（`set_job_assigned_node_for_failover`） |
| “Job does not exist” 日志 | `central_server/scheduler/src/websocket/node_handler/message/job_result/job_result_job_management.rs`（`check_should_process_job`） |
| 已完成 Job 清理 | `central_server/scheduler/src/core/dispatcher/job_management.rs`（`cleanup_completed_jobs`）、`src/app/startup.rs`（定时调用） |

---

**结论**：  
- “一直在重新分配任务” 需用日志中的 **failover/重派/超时** 来确认；若无，则多为多 Job 连续派发。  
- “Job does not exist” 多为 **结果在 Job 被清理之后才到达**（或重复结果）；当前逻辑仍会把结果入队，不影响按 utterance 的连续性，可结合节点上报时机与清理策略进一步优化。
