//! Silero VAD 语音活动检测引擎
//! 
//! 使用 ONNX Runtime 加载和运行 Silero VAD 模型，支持 GPU 加速
//! 
//! 参考原项目实现（D:\Programs\github\lingua/core/engine/src/vad/silero_vad.rs）
//! 实现节点端 Level 2 VAD，用于拼接音频块后的断句

use anyhow::{Result, anyhow};
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::sync::Mutex;
use std::collections::VecDeque;
use ort::{
    Environment,
    session::{Session, SessionBuilder},
    Tensor,
    execution_providers::ExecutionProvider,
};
use ndarray::{Array1, Array2, Array3, CowArray, Ix2, Ix3};
use tracing::info;

/// Silero VAD 配置
#[derive(Debug, Clone)]
pub struct VADConfig {
    /// 采样率（Silero VAD 要求 16kHz）
    pub sample_rate: u32,
    /// 帧大小（512 samples @ 16kHz = 32ms）
    pub frame_size: usize,
    /// 静音阈值（0.0-1.0），低于此值认为是静音
    pub silence_threshold: f32,
    /// 最小静音时长（毫秒），超过此时长才判定为自然停顿
    pub min_silence_duration_ms: u64,
    /// 是否启用自适应调整
    pub adaptive_enabled: bool,
    /// 基础阈值范围（毫秒）
    pub base_threshold_min_ms: u64,
    pub base_threshold_max_ms: u64,
    /// 最终阈值范围（毫秒）
    pub final_threshold_min_ms: u64,
    pub final_threshold_max_ms: u64,
    /// 最小话语时长（防止半句话被切掉，毫秒）
    pub min_utterance_ms: u64,
}

impl Default for VADConfig {
    fn default() -> Self {
        Self {
            sample_rate: 16000,
            frame_size: 512,  // 32ms @ 16kHz
            silence_threshold: 0.2,  // 降低阈值，提高语音检测灵敏度
            min_silence_duration_ms: 300,  // 基础阈值
            adaptive_enabled: true,
            base_threshold_min_ms: 200,
            base_threshold_max_ms: 600,
            final_threshold_min_ms: 200,
            final_threshold_max_ms: 800,
            min_utterance_ms: 1000,
        }
    }
}

/// 自适应状态（根据语速动态调整阈值）
struct AdaptiveState {
    /// 语速历史（字符/秒）
    speech_rate_history: VecDeque<f32>,
    /// 基础阈值（由语速自适应生成，毫秒）
    base_threshold_ms: u64,
    /// 样本数量
    sample_count: usize,
}

impl AdaptiveState {
    fn new(base_duration_ms: u64) -> Self {
        Self {
            speech_rate_history: VecDeque::with_capacity(20),
            base_threshold_ms: base_duration_ms,
            sample_count: 0,
        }
    }
    
    /// 更新语速并调整阈值
    fn update_speech_rate(&mut self, speech_rate: f32, config: &VADConfig) {
        self.speech_rate_history.push_back(speech_rate);
        if self.speech_rate_history.len() > 20 {
            self.speech_rate_history.pop_front();
        }
        self.sample_count += 1;
        
        // 计算平均语速（使用指数加权移动平均）
        let avg_speech_rate = if !self.speech_rate_history.is_empty() {
            let alpha = 0.5;
            let mut weighted_sum = 0.0;
            let mut weight_sum = 0.0;
            let history_len = self.speech_rate_history.len();
            for (i, &rate) in self.speech_rate_history.iter().enumerate() {
                let weight = (1.0_f32 - alpha).powi((history_len - i - 1) as i32);
                weighted_sum += rate * weight;
                weight_sum += weight;
            }
            weighted_sum / weight_sum
        } else {
            speech_rate
        };
        
        // 根据语速动态计算阈值倍数（使用 sigmoid 函数）
        let sigmoid = |x: f32| -> f32 {
            1.0 / (1.0 + (-x).exp())
        };
        
        let normalized_rate = (avg_speech_rate - 6.0) / 2.0;
        let sigmoid_value = sigmoid(normalized_rate);
        let multiplier = 1.0 + (0.5 - sigmoid_value) * 0.4;
        let multiplier = multiplier.clamp(0.5, 1.5);
        
        // 应用调整
        let base_threshold_center = (config.base_threshold_min_ms + config.base_threshold_max_ms) / 2;
        let target_base = (base_threshold_center as f32 * multiplier) as u64;
        let adjustment = (target_base as f32 - self.base_threshold_ms as f32) * 0.4;
        self.base_threshold_ms = ((self.base_threshold_ms as f32 + adjustment) as u64)
            .clamp(config.base_threshold_min_ms, config.base_threshold_max_ms);
    }
    
    /// 获取有效阈值
    fn get_effective_threshold(&self, config: &VADConfig) -> u64 {
        self.base_threshold_ms.clamp(config.final_threshold_min_ms, config.final_threshold_max_ms)
    }
    
    /// 获取调整后的阈值
    fn get_adjusted_duration(&self, config: &VADConfig) -> u64 {
        if self.sample_count == 0 {
            config.min_silence_duration_ms
        } else {
            self.get_effective_threshold(config)
        }
    }
}

/// Silero VAD 引擎
pub struct VADEngine {
    session: Arc<Mutex<Session>>,
    model_path: PathBuf,
    config: VADConfig,
    /// 隐藏状态（用于 VAD 模型的状态传递）
    hidden_state: Arc<Mutex<Option<Array2<f32>>>>,
    /// 连续静音帧数
    silence_frame_count: Arc<Mutex<usize>>,
    /// 上一个检测到语音的时间戳
    last_speech_timestamp: Arc<Mutex<Option<u64>>>,
    /// 自适应状态
    adaptive_state: Arc<Mutex<AdaptiveState>>,
    /// 上一次边界检测的时间戳（用于冷却期）
    last_boundary_timestamp: Arc<Mutex<Option<u64>>>,
    /// 帧缓冲区（用于累积小帧）
    frame_buffer: Arc<Mutex<Vec<f32>>>,
}

impl VADEngine {
    /// 从模型目录加载 Silero VAD 模型
    pub fn new(model_dir: PathBuf) -> Result<Self> {
        let possible_names = ["silero_vad_official.onnx", "silero_vad.onnx", "model.onnx"];
        let model_path = possible_names.iter()
            .find_map(|name| {
                let path = model_dir.join(name);
                if path.exists() {
                    Some(path)
                } else {
                    None
                }
            })
            .ok_or_else(|| anyhow!(
                "No Silero VAD model file found in directory: {}. Tried: {:?}",
                model_dir.display(),
                possible_names
            ))?;

        Self::new_from_model_path(&model_path, VADConfig::default())
    }

    /// 从模型文件路径加载 Silero VAD 模型
    pub fn new_from_model_path(model_path: &Path, config: VADConfig) -> Result<Self> {
        if !model_path.exists() {
            return Err(anyhow!("Model file not found: {}", model_path.display()));
        }

        info!("Loading Silero VAD model from: {}", model_path.display());

        // 初始化 ONNX Runtime 环境（ort crate 2.0）
        let env = Arc::new(
            Environment::builder()
                .with_name("silero_vad")
                .build()?
        );

        // 创建会话，优先使用 CUDA（如果可用）
        // ort crate 2.0: 使用 SessionBuilder::new(&env) 和 commit() 方法
        let session = match SessionBuilder::new(&env)?
            .with_execution_providers([ExecutionProvider::CUDA(Default::default())])?
            .with_model_from_file(model_path)?
            .commit() {
            Ok(sess) => {
                info!("Silero VAD: Using CUDA GPU acceleration");
                sess
            }
            Err(e) => {
                info!("Silero VAD: CUDA not available, falling back to CPU: {}", e);
                // 如果 CUDA 失败，重新创建 session（不使用 CUDA）
                SessionBuilder::new(&env)?
                    .with_model_from_file(model_path)?
                    .commit()?
            }
        };

        info!("Silero VAD model loaded successfully");

        let base_threshold = (config.base_threshold_min_ms + config.base_threshold_max_ms) / 2;

        Ok(Self {
            session: Arc::new(Mutex::new(session)),
            model_path: model_path.to_path_buf(),
            config,
            hidden_state: Arc::new(Mutex::new(None)),
            silence_frame_count: Arc::new(Mutex::new(0)),
            last_speech_timestamp: Arc::new(Mutex::new(None)),
            adaptive_state: Arc::new(Mutex::new(AdaptiveState::new(base_threshold))),
            last_boundary_timestamp: Arc::new(Mutex::new(None)),
            frame_buffer: Arc::new(Mutex::new(Vec::new())),
        })
    }

    /// 检测语音活动（用于拼接后的音频块）
    /// 
    /// # Arguments
    /// * `audio_data` - 音频数据（f32，16kHz，单声道，范围 -1.0 到 1.0）
    /// 
    /// # Returns
    /// 返回语音段的起止位置列表（样本索引）
    pub fn detect_speech(&self, audio_data: &[f32]) -> Result<Vec<(usize, usize)>> {
        let mut segments = Vec::new();
        let mut current_segment_start: Option<usize> = None;
        
        for (frame_idx, frame) in audio_data.chunks(self.config.frame_size).enumerate() {
            if frame.len() < self.config.frame_size {
                break;
            }

            let speech_prob = self.detect_voice_activity_frame(frame)?;
            
            if speech_prob > self.config.silence_threshold {
                let sample_start = frame_idx * self.config.frame_size;
                if current_segment_start.is_none() {
                    current_segment_start = Some(sample_start);
                }
            } else {
                if let Some(start) = current_segment_start {
                    let sample_end = frame_idx * self.config.frame_size;
                    segments.push((start, sample_end));
                    current_segment_start = None;
                }
            }
        }

        if let Some(start) = current_segment_start {
            segments.push((start, audio_data.len()));
        }

        Ok(segments)
    }

    /// 检测单帧的语音活动概率
    fn detect_voice_activity_frame(&self, audio_frame: &[f32]) -> Result<f32> {
        if audio_frame.len() != self.config.frame_size {
            return Err(anyhow!(
                "Audio frame length {} does not match frame size {}",
                audio_frame.len(),
                self.config.frame_size
            ));
        }

        // 归一化到 [-1, 1]
        let normalized: Vec<f32> = audio_frame.iter()
            .map(|&x| x.clamp(-1.0, 1.0))
            .collect();

        // 创建音频输入数组（形状：[1, frame_size]）
        let input_array = Array2::from_shape_vec((1, normalized.len()), normalized)
            .map_err(|e| anyhow!("Failed to create input array: {}", e))?;

        // 获取或初始化隐藏状态（形状：[2, 1, 128]）
        let state_array = {
            let mut state_guard = self.hidden_state.lock()
                .map_err(|e| anyhow!("Failed to lock hidden state: {}", e))?;
            
            if let Some(ref state_2d) = *state_guard {
                state_2d.clone().into_shape((2, 1, 128))
                    .map_err(|e| anyhow!("Failed to reshape state: {}", e))?
            } else {
                let new_state = Array3::<f32>::zeros((2, 1, 128));
                *state_guard = Some(new_state.clone().into_shape((2, 128))
                    .map_err(|e| anyhow!("Failed to reshape new state: {}", e))?);
                new_state
            }
        };

        // 转换为动态维度
        let arr_dyn = input_array.into_dyn();
        let arr_owned = arr_dyn.to_owned();
        let cow_arr = CowArray::from(arr_owned);

        let state_dyn = state_array.into_dyn();
        let state_owned = state_dyn.to_owned();
        let state_cow = CowArray::from(state_owned);

        // 创建采样率输入（Int64 标量）
        let sr_array = Array1::from_vec(vec![self.config.sample_rate as i64]);
        let sr_dyn = sr_array.into_dyn();
        let sr_owned = sr_dyn.to_owned();
        let sr_cow = CowArray::from(sr_owned);

        // 创建 ONNX 输入（ort crate 2.0: 使用 Tensor::from_array 和 ort::inputs! 宏）
        let audio_tensor = Tensor::from_array(cow_arr.into_owned())
            .map_err(|e| anyhow!("Failed to create audio input: {}", e))?;
        
        let state_tensor = Tensor::from_array(state_cow.into_owned())
            .map_err(|e| anyhow!("Failed to create state input: {}", e))?;
        
        let sr_tensor = Tensor::from_array(sr_cow.into_owned())
            .map_err(|e| anyhow!("Failed to create sr input: {}", e))?;

        // 推理（ort crate 2.0: 使用 ort::inputs! 宏）
        let session_guard = self.session.lock()
            .map_err(|e| anyhow!("Failed to lock session: {}", e))?;
        
        let outputs = session_guard
            .run(ort::inputs![audio_tensor, state_tensor, sr_tensor]?)
            .map_err(|e| anyhow!("ONNX inference failed: {}", e))?;

        // 提取输出（ort crate 2.0 API: 使用 try_extract_array）
        use ndarray::IxDyn;

        // ort crate 2.0: outputs 可以通过索引访问
        let output_value = &outputs[0];
        let output_array: ndarray::ArrayViewD<f32> = output_value
            .try_extract_array()
            .map_err(|e| anyhow!("Failed to extract output: {}", e))?;

        // 更新隐藏状态
        if outputs.len() > 1 {
            let state_value = &outputs[1];
            let state_array: ndarray::ArrayViewD<f32> = state_value
                .try_extract_array()
                .map_err(|e| anyhow!("Failed to extract state: {}", e))?;
            
            let new_state_3d: Array3<f32> = state_array
                .to_owned()
                .into_dimensionality::<Ix3>()
                .map_err(|e| anyhow!("Failed to reshape state: {}", e))?;
            
            let new_state_2d = new_state_3d.into_shape((2, 128))
                .map_err(|e| anyhow!("Failed to reshape state for storage: {}", e))?;
            
            let mut state_guard = self.hidden_state.lock()
                .map_err(|e| anyhow!("Failed to lock hidden state: {}", e))?;
            *state_guard = Some(new_state_2d);
        }

        // 提取输出值
        let view = output_array;
        let shape = view.shape();
        
        let raw_output = if shape.len() == 2 {
            let output_array: Array2<f32> = view
                .to_owned()
                .into_dimensionality::<Ix2>()
                .map_err(|e| anyhow!("Failed to reshape output: {}", e))?;
            
            if output_array.shape()[1] >= 2 {
                output_array[[0, 1]]  // 第二列是语音概率
            } else {
                output_array[[0, 0]]
            }
        } else if shape.len() == 1 {
            let output_array: Array1<f32> = view
                .to_owned()
                .into_dimensionality::<ndarray::Ix1>()
                .map_err(|e| anyhow!("Failed to reshape output: {}", e))?;
            output_array[0]
        } else {
            let flat: Vec<f32> = view.iter().copied().collect();
            if flat.is_empty() {
                return Err(anyhow!("Output tensor is empty"));
            }
            flat[0]
        };
        
        // 处理输出值（参考原项目实现）
        let speech_prob = if raw_output < -10.0 || raw_output > 10.0 {
            // logit，使用 sigmoid 转换
            1.0 / (1.0 + (-raw_output).exp())
        } else if raw_output < 0.2 && raw_output > -0.01 {
            // 小值，需要乘以系数后再应用 sigmoid
            let scaled_logit = raw_output * 10.0;
            1.0 / (1.0 + (-scaled_logit).exp())
        } else if raw_output < 0.5 {
            // 可能是静音概率，取反
            1.0 - raw_output
        } else {
            // 已经是语音概率
            raw_output
        };
        
        Ok(speech_prob)
    }

    /// 更新语速（用于自适应调整）
    /// 
    /// # Arguments
    /// * `text` - 识别的文本
    /// * `audio_duration_ms` - 音频时长（毫秒）
    pub fn update_speech_rate(&self, text: &str, audio_duration_ms: u64) {
        if !self.config.adaptive_enabled || audio_duration_ms == 0 {
            return;
        }

        let text_length = text.chars().count() as f32;
        let audio_duration_sec = audio_duration_ms as f32 / 1000.0;
        let speech_rate = text_length / audio_duration_sec;

        // 检查语速是否在合理范围内
        const MIN_REASONABLE_RATE: f32 = 0.5;
        const MAX_REASONABLE_RATE: f32 = 50.0;
        
        if speech_rate < MIN_REASONABLE_RATE || speech_rate > MAX_REASONABLE_RATE {
            return;
        }

        let mut state = self.adaptive_state.lock().unwrap();
        state.update_speech_rate(speech_rate, &self.config);
    }

    /// 获取调整后的阈值
    pub fn get_adjusted_duration_ms(&self) -> u64 {
        if !self.config.adaptive_enabled {
            return self.config.min_silence_duration_ms;
        }
        
        let state = self.adaptive_state.lock().unwrap();
        state.get_adjusted_duration(&self.config)
    }

    /// 重置状态（用于新的音频流）
    pub fn reset_state(&self) -> Result<()> {
        let mut state_guard = self.hidden_state.lock()
            .map_err(|e| anyhow!("Failed to lock hidden state: {}", e))?;
        *state_guard = None;
        
        let mut silence_count = self.silence_frame_count.lock()
            .map_err(|e| anyhow!("Failed to lock silence_frame_count: {}", e))?;
        *silence_count = 0;
        
        let mut last_speech = self.last_speech_timestamp.lock()
            .map_err(|e| anyhow!("Failed to lock last_speech_timestamp: {}", e))?;
        *last_speech = None;
        
        let mut last_boundary = self.last_boundary_timestamp.lock()
            .map_err(|e| anyhow!("Failed to lock last_boundary_timestamp: {}", e))?;
        *last_boundary = None;
        
        let mut buffer = self.frame_buffer.lock()
            .map_err(|e| anyhow!("Failed to lock frame_buffer: {}", e))?;
        buffer.clear();
        
        let base_threshold = (self.config.base_threshold_min_ms + self.config.base_threshold_max_ms) / 2;
        let mut adaptive = self.adaptive_state.lock()
            .map_err(|e| anyhow!("Failed to lock adaptive_state: {}", e))?;
        *adaptive = AdaptiveState::new(base_threshold);
        
        Ok(())
    }

    /// 获取模型路径
    pub fn model_path(&self) -> &Path {
        &self.model_path
    }

    /// 设置静音阈值
    pub fn set_silence_threshold(&mut self, threshold: f32) {
        self.config.silence_threshold = threshold.clamp(0.0, 1.0);
    }

    /// 获取当前静音阈值
    pub fn silence_threshold(&self) -> f32 {
        self.config.silence_threshold
    }
}
