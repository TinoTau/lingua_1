# Semantic Repair 英文服务修复说明

## 修复内容

已按照中文服务的修复方案，将英文服务从 `transformers + BitsAndBytes` 迁移到 `llama.cpp + GGUF`。

## 主要修改

### 1. 创建 llama.cpp 引擎适配器
- **文件**: `llamacpp_engine.py`
- **功能**: 使用 llama.cpp 进行推理，支持 GPU 加速
- **特点**: 使用 chat completion 格式（Qwen2.5-Instruct 是 chat 模型）

### 2. 修改模型加载器
- **文件**: `model_loader.py`
- **修改**: 
  - 移除 transformers 相关代码
  - 添加 `find_gguf_model_path()` 函数
  - 支持查找英文 GGUF 模型，如果没有则回退到中文模型（Qwen2.5 支持多语言）

### 3. 修改主服务文件
- **文件**: `semantic_repair_en_service.py`
- **修改**:
  - 移除 `transformers`、`BitsAndBytes` 相关导入
  - 使用 `LlamaCppEngine` 替代 `RepairEngine`
  - 更新启动逻辑使用 GGUF 模型
  - 更新健康检查端点

## 模型路径

服务会按以下顺序查找模型：

1. **优先**: `models/qwen2.5-3b-instruct-en-gguf/qwen2.5-3b-instruct-q4_k_m.gguf`
2. **回退**: `models/qwen2.5-3b-instruct-zh-gguf/qwen2.5-3b-instruct-q4_k_m.gguf`（Qwen2.5 支持多语言）

## 优势

1. **避免依赖冲突**: 不再依赖 `transformers`、`auto-gptq`、`BitsAndBytes`
2. **性能提升**: GPU 模式性能提升约 20 倍
3. **资源占用低**: GGUF 4bit 量化，常驻资源低
4. **启动更快**: 模型加载时间显著减少

## 注意事项

1. **模型文件**: 如果没有英文专用 GGUF 模型，服务会自动使用中文模型（Qwen2.5 支持英文）
2. **GPU 支持**: 需要已安装带 CUDA 支持的 `llama-cpp-python`（与中文服务共享）
3. **端口**: 英文服务使用端口 `5011`（中文服务使用 `5013`）

## 测试

启动服务后，可以运行：

```powershell
# 健康检查
Invoke-RestMethod -Uri "http://127.0.0.1:5011/health"

# 测试修复
$body = @{job_id="test";session_id="test";text_in="Hello, this is a test.";lang="en"} | ConvertTo-Json
Invoke-RestMethod -Uri "http://127.0.0.1:5011/repair" -Method Post -Body $body -ContentType "application/json"
```
