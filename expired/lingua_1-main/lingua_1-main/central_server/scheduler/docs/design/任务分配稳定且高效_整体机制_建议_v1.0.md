# 任务分配稳定且高效的整体机制建议（面向生产环境）v1.0

> 适用对象：Scheduler/Dispatcher/会话音频链路（WebSocket Session）  
> 目标：在不牺牲吞吐的前提下，消除“重复 finalize / 重复创建 job / 结果队列阻塞”等竞态，形成可平移到多实例/生产环境的机制。  
> 背景：当前 Scheduler 同时承担 **WebSocket 会话入口 + 节点入口 + 任务下发/结果回传**，属于“控制面 + 连接面（部分数据面）”合体；因此在启用多实例前，需要先把单实例内部并发与幂等做成可证明正确的闭环，再扩展到 Redis/多实例投递。

---

## 1. 现象与根因归纳（结合现有代码与报告）

### 1.1 已观察到的典型症状
- 同一 `utterance_index` 被多次 finalize（触发原因可能是 `pause_exceeded`、`is_final`、timeout task 并发触发）。
- 因为 finalize 重入导致 **重复创建 job**、或 **某些 index job 未创建**，进而 **结果队列等待缺失 index 阻塞**。
- 日志中可见同一 index 多次“Finalizing … reason=Pause”等行为，以及后续“Incremented utterance_index …”出现顺序交错。

### 1.2 现有“轻量去重检查”的有效性边界
当前修复建议是在 `finalize_audio_utterance` 开头加入：
- `if session.utterance_index > utterance_index { return Ok(false); }`

该检查能在 **index 已经完成并递增** 后阻止后续重复 finalize，但它无法阻止“在 index 递增之前”的并发 finalize（因为多个并发分支可能几乎同时读取到相同的 `session.utterance_index` 快照）。

同时，`take_combined(sess_id, utterance_index)` 会把缓冲区“取走”（消耗式读取），因此即使重复 finalize 没有生成重复 job，也可能出现：
- 先到的 finalize 取走 audio buffer；后到的 finalize 读不到 buffer → 返回 false；
- 若并发路径中还有其它“取走/递增”的交错，可能造成某些 index 被跳过，从而结果队列等待缺口而阻塞。

结论：**需要从“可证明的并发控制”层面消除 finalize 重入，而不是仅在末端做跳过。**

---

## 2. 你想要的目标：稳定且高效（不是二选一）

要做到“稳定且高效”，核心思想是：
1) **把会话内的并发改为顺序化（Single Writer）**：同一 session 的音频分段、计时器触发、finalize、job 创建、结果推送等事件必须由一个“会话内串行执行体”处理。  
2) **把跨会话并行保留**：不同 session 之间并行，不要用全局大锁拖垮吞吐。  
3) **把幂等放在边界**：job 创建与派发必须有明确的幂等键与一次性语义，避免重试/网络抖动造成重复负载。

这三点组合，可以同时实现：
- 稳定：无竞态、无重复、无跳号、结果队列不会因内部 bug 卡死；
- 高效：跨会话并行；会话内锁粒度极小；避免大量无效的重算/重试。

---

## 3. 推荐的“一步到位”架构机制：Session Actor（单写者）+ 幂等 Job 边界

### 3.1 Session Actor：为每个 session 建立“事件循环”（建议优先方案）
为每个 session 建一个轻量 actor（Tokio task），所有与该 session 相关的事件都通过 channel 投递给它处理：
- `AudioChunkReceived(chunk)`
- `PauseExceeded(now)`
- `IsFinalReceived()`
- `TimeoutFired(generation)`
- `NodeResultReceived(utterance_index, …)`
- `CancelTimers / ResetTimers`
- `CloseSession`

**关键点：同一 session 的状态只能由该 actor 修改。**  
它天然满足：
- `utterance_index` 单调递增，且只在“成功 finalize”后递增；
- `take_combined` 与 `increment` 不会被并发打断；
- 计时器的触发不会与 finalize 重入（因为触发也只是发消息到 actor 队列）。

> 这不是“止血”；这是把并发错误从根上移除，且不会降低吞吐：系统吞吐由“并发 session 数量”决定，而不是由“单 session 内的并发分支”决定。

#### 3.1.1 事件循环的状态机（建议）
每个 session actor 维护：
- `current_utterance_index: u64`
- `buffer: Vec<i16/bytes>`（或引用 audio_buffer 的 session-local 分片）
- `state: Idle | Collecting | Finalizing{index} | Closed`
- `finalize_inflight: bool`（或用状态机表达）
- `timer_generation: u64`（用于判定 timer 是否过期）
- `result_reorder_buffer`（用于处理 out-of-order results）

规则：
- `Collecting` 时接受音频 chunk，累积 buffer。
- 触发 finalize（Pause/Send/Timeout/IsFinal）时：
  - 若 `state == Finalizing`：丢弃或合并本次 finalize 请求（例如记录“也触发过 Pause，但已在 Finalizing”）。
  - 若 `buffer.is_empty()`：忽略 finalize。
  - 否则进入 `Finalizing{index=current_utterance_index}`，创建 jobs，并在 **job 创建成功入队/派发成功（或至少创建成功）** 后将 `current_utterance_index += 1`，然后回到 `Collecting`。
- 任何定时器触发都必须携带 `timer_generation`，actor 只接受与当前 generation 匹配的触发（旧 timer 触发视为过期）。

#### 3.1.2 计时器处理：从“并发任务”改为“可取消的 generation token”
问题中提到 timeout task 与 pause_exceeded、is_final 竞态。标准做法：
- actor 每次收到音频 chunk 时，更新 `timer_generation += 1`，并启动（或重置）一个 timer：
  - timer 到期后发送 `TimeoutFired(generation)` 给 actor。
- actor 收到 `TimeoutFired(gen)` 时：
  - 若 `gen != current_generation`，说明是旧 timer，直接忽略；
  - 否则走 finalize（Pause/Timeout）。

该模式比“spawn 一个不可控的 async task 并在多个分支 finalize”更稳定，也更省资源（不会堆积大量 timer task）。

---

### 3.2 Job 创建与派发边界：必须“幂等键 + 一次性语义”

即使 session actor 完美串行，也仍需处理：
- 网络重试、节点断线重连、Scheduler 重启恢复、跨实例转发重复投递等带来的重复消息。

因此必须在“job 创建/派发”边界设立幂等机制，建议统一使用以下幂等键：

**Idempotency Key：**  
`job_key = {tenant_id}:{session_id}:{utterance_index}:{job_type}:{tgt_lang}:{features_hash}`

要求：
1) **创建 job**：同一个 `job_key` 只能创建一次（重复调用返回同一 job_id）。  
2) **派发 job**：同一个 job_id 只能被“成功派发”一次（重复派发不应重复占用节点 capacity）。

你们 Phase 1/2 文档里已经引入“任务级幂等 request_id + lease”的思路，建议把 `job_key` 作为 request_id 的上层定义，避免各模块各自定义 idempotency 口径。

---

## 4. 多实例（生产）扩展路线：先内部正确，再跨实例可用

### 4.1 为什么“仅加 Redis”不够
当前 Scheduler 还持有 WebSocket 连接（session 与 node）。多实例下会出现：
- A 实例做调度选中 node X，但 node X 的连接在 B → A 无法下发；
- node 回传在 B，但 session 在 A → 无法推送结果。

因此多实例要可用，必须引入：
- **连接归属（owner）**：`nodes:owner:<node_id> -> instance_id`、`sessions:owner:<session_id> -> instance_id`（带 TTL）
- **跨实例投递通道**：按 instance_id 分流的投递（Pub/Sub 简单低延迟；Streams 可重放更可靠但实现更复杂）

> 这与 Phase 2 推进建议一致：多实例的前置是“连接归属 + 跨实例投递”，否则链路会断。

### 4.2 Session Actor 与多实例的关系（推荐组合）
- session actor 仍然存在于“持有 session 连接”的实例上；
- node 连接同理存在于“持有 node 连接”的实例上；
- 当 actor 需要向某 node 下发 job：
  - 如果 node owner == 本实例：直接 WebSocket send；
  - 否则：把 `DispatchJob(job_id, payload)` 投递到 node owner 实例的投递通道。
- 当 node 回传结果：
  - 如果 session owner == 本实例：直接 push；
  - 否则投递到 session owner 实例。

这样可以做到：
- 单机多进程（2–4 个 Scheduler 副本）即可验证生产级的“多实例正确性”；
- 同机验证后可平移到多机 + LB。

---

## 5. “稳定且高效”的关键工程约束（规范化）

### 5.1 会话内单写者约束（强制）
- 禁止任何会话内状态（buffer、utterance_index、result queue pointer）在多个 async 分支直接读写。
- 允许的并发：不同 session 并发；同一 session 串行。

### 5.2 音频缓冲区 ownership（强制）
- `take_combined` 必须只被 actor 调用，且在 finalize 成功路径中调用一次。
- 如果仍使用全局 `AudioBuffer` 结构：建议将其内部改为 `HashMap<session_id, SessionAudioBuffer>`，并提供“仅 actor 可用”的 API，避免被其它路径误调用。

### 5.3 结果队列的“抗缺口”策略（建议）
即使系统内部正确，也可能因节点故障出现缺口（某个 utterance_index 永远失败/超时）。若结果队列严格等待连续 index，会造成“后续结果永远不推送”。

建议：
- 对每个 utterance_index 设置 `result_deadline`（例如 30–60s）。
- 超过 deadline 未到达：
  - 发送 UI 事件/错误事件（该句失败），并 **推进水位线** 允许后续结果继续发送；
  - 或者将该 index 标记为 `skipped`，避免永久阻塞。

该策略属于生产必备：它把“单点失败”限制为“单句失败”，不会演变成“整会话卡死”。

### 5.4 稳定性与效率指标（建议验收口径）
- 稳定性：
  - 单 session 内：utterance_index 单调递增，且每个 index 至多 finalize 一次；
  - 任何触发源（Pause/Send/Timeout/IsFinal）都不会造成重复 job 创建；
  - 结果队列不会因缺口永久阻塞（有明确 skip/timeout 行为）。
- 效率：
  - 同机 2–4 个 Scheduler 副本时，吞吐随副本数线性提升（考虑 node 数量与网络瓶颈）；
  - Scheduler CPU/内存随 session 数增长平滑，不出现大量 timer task 堆积。

---

## 6. 实现步骤（建议按“先正确，再扩展”落地）

### Step 1：引入 Session Actor（单实例先落地）
1) 为 `SessionManager` 增加 `SessionActorHandle` 注册表（session_id -> sender）。
2) WebSocket 收到音频 chunk 时，不再直接触发 finalize/创建 job；而是：
   - `actor_sender.send(AudioChunkReceived(chunk))`
3) Pause/timeout/is_final 触发时，不再调用 finalize；而是：
   - `actor_sender.send(FinalizeRequested(reason))` 或 `TimeoutFired(generation)`
4) 把 `finalize_audio_utterance` 的逻辑“内联”到 actor 中（或者保留函数，但只能在 actor 里调用）。

### Step 2：统一幂等键（单实例即可收益）
1) 在 Dispatcher / JobCreator 增加 `job_key -> job_id` 的映射（进程内 + TTL）。
2) job 创建时先查映射，已存在则复用 job_id。
3) 派发标记 `dispatched_to_node` 也应具备幂等（重复标记无副作用）。

### Step 3：结果队列抗缺口（生产必选）
1) 为每个 index 增加 deadline/timeout。
2) 超时后 emit error + 推进水位线。
3) 增加监控：缺口数量、超时数量、平均等待时间。

### Step 4：Redis 化（Phase 2 A：状态外置 + 原子一致）
1) 把关键状态（bind、capacity、request_id/lease、风暴去抖 key）迁移到 Redis。
2) 原子一致用 Lua 合并提交；Cluster 环境下 key 必须同 slot（hash tag）。
3) 单实例运行即可验证正确性与一致性。

### Step 5：多实例可用（Phase 2 B：owner + 跨实例投递）
1) session/node owner 写入 Redis（TTL）。
2) 实例间投递通道：优先 Pub/Sub（快且实现简单）；若需要可重放，改 Streams。
3) 在同机跑 2–4 副本 + 本机 LB 验证全链路。

---

## 7. 风险与取舍（让决策部门可拍板）

### 7.1 Session Actor 的成本
- 代码结构调整较大：需要把“会话内逻辑”集中到一个地方。
- 但收益是确定性的：消除竞态类 bug 的根因，降低运维与线上事故成本。

### 7.2 Pub/Sub vs Streams
- Pub/Sub：低延迟、实现简单；缺点是不可重放（实例重启时可能丢消息）。
- Streams：可重放、消费组；缺点是实现复杂度上升，需要 ack/重试/积压治理。

建议：**先 Pub/Sub 验证链路，后续如需更强可靠性再迁移 Streams。**

---

## 8. 工作量粗估（按你们当前 Phase 1/2 范围）

> 以下为工程量级估算（人日），假设 1 名熟悉 Rust/Tokio 的后端工程师；如多人并行可压缩日历时间。

### 8.1 Session Actor（核心改造）
- 事件定义、actor 框架、接入 WebSocket 消息流：2–3 人日  
- 迁移音频分段/计时器/finalize/job 创建到 actor：3–5 人日  
- 基础回归测试与压测脚本（并发、竞态）：2–3 人日  
**小计：7–11 人日**

### 8.2 幂等键统一 + 派发一次性语义
- job_key 设计与实现、TTL 清理：1–2 人日  
- 派发标记幂等与回归：1 人日  
**小计：2–3 人日**

### 8.3 结果队列抗缺口（避免整会话卡死）
- deadline/skip 机制 + UI 事件：1–2 人日  
- 指标与告警：1 人日  
**小计：2–3 人日**

### 8.4 Redis Phase 2 A（状态外置 + Lua 原子一致）
- key 设计（含 hash tag）、Lua、单机 cluster 部署脚本：3–5 人日  
- 压测与一致性验证：2–3 人日  
**小计：5–8 人日**

### 8.5 多实例 Phase 2 B（owner + 跨实例投递）
- owner 表 + TTL 续租：1–2 人日  
- Pub/Sub 投递通道与本地投递 worker：3–5 人日  
- 同机 2–4 副本 + LB 灰度验证：2–3 人日  
**小计：6–10 人日**

**总计（面向生产一步到位，含多实例能力）：22–35 人日**  
如果仅追求“单实例稳定且高效（消除竞态）”，可先交付 Step 1–3：约 **11–17 人日**。

---

## 9. 建议的决策结论（可直接用于评审会议）

1) **采用 Session Actor（会话内单写者）作为“稳定且高效”的根机制**：它是消除竞态与重复 job 的唯一低风险路径。  
2) **将幂等键上收为统一标准（job_key/request_id）**：为重试、跨实例投递、节点抖动提供一次性语义。  
3) **结果队列必须具备抗缺口能力**：生产环境不可接受“缺一条结果就整会话永久阻塞”。  
4) 在此基础上，按 Phase 2 路线推进：
   - A：Redis 状态外置 + Lua 原子一致（单实例也受益）
   - B：owner + 跨实例投递（多实例可用的前置）
   - C：LB + 多副本运行（同机验证后平移多机）

---

## 10. 附：对“稳定模式 / 高效模式”的定位建议（如仍需提供给用户选择）

在上述架构下，两者可以体现在 **策略参数**，而不是体现在“是否正确”：
- 稳定模式：更保守的 retry/backoff、更长的 lease、更严格的 capacity 约束、更短的并发窗口；
- 高效模式：更激进的候选节点扩展、并行派发（但仍保持幂等），更短的 timeout 以快速 failover。

注意：**正确性（不重复、不跳号、不阻塞）不应成为可选项**，它应是底座能力。

