# 0. 清理日志
cd D:\Programs\github\lingua_1
.\scripts\cleanup_orphaned_processes.ps1
.\scripts\clear_python_cache.ps1
.\clear_logs.ps1

# 1. 节点端
cd D:\Programs\github\lingua_1\electron_node\electron-node
npm run clear-cache
npm run build
cd D:\Programs\github\lingua_1
.\scripts\start_electron_node.ps1

cd D:\Programs\github\lingua_1\electron_node\services\semantic_repair_zh
.\start_all_in_one.ps1


# 2. 调度服务器（在新窗口）
cd D:\Programs\github\lingua_1\central_server\scheduler
cargo clean
cd D:\Programs\github\lingua_1
.\scripts\start_scheduler.ps1

# 3. 模型管理
cd D:\Programs\github\lingua_1
.\scripts\start_model_hub.ps1

# redis 集群
cd D:\Programs\github\lingua_1\central_server\scheduler
.\scripts\phase2_cluster_acceptance.ps1

# 在仓库根目录执行
cd D:\Programs\github\lingua_1
.\scripts\start_all_services_with_terminals.ps1


访问仪表盘：
http://localhost:5010/dashboard
查看统计数据 API：
http://localhost:5010/api/v1/stats


# 4. Web 客户端（在新窗口）
cd D:\Programs\github\lingua_1\webapp\web-client
.\clean_cache.ps1
npm install
npm run build
cd D:\Programs\github\lingua_1
.\scripts\start_webapp.ps1

#导出日志：http://localhost:9001/export-logs-simple.html



cd D:\Programs\github\lingua_1\electron_node\electron-node 
Get-Location   
npm run build:main  


1.语义修复服务合并
2.embedding实装，服务模式+连续播放
3.yourtts实装
1.model hub改造成多实例
2.日志服务器，统计节点性能，错误日志
3.公司指定服务器
4.会议室实装，连续播放
4.第三方API，SDK
4.收费模式
5.模型训练

请将D:\Programs\github\lingua_1\electron_node的docs文档进行整理，根据实际代码补充新增功能，移除过期内容，并能合并的就合并，文档大小最好不要超过500行。测试报告之类的文件全都移除

现在请对D:\Programs\github\lingua_1\electron_node\services\faster_whisper_vad的代码文件进行整理，拆分超过500行的大文件，合并重复的逻辑，只对代码进行迁移，不要改变任何接口，参数和返回结果，也不要改动任何代码逻辑。异步方法之间如果有必要的话采用状态机进行管理，然后把Import都移动到文件头部

请为实现这个功能，添加流程日志，进行单元测试，并更新文档。

不要考虑兼容，这个项目没有上线，没有用户，只需要保持代码简洁，做好单元测试，然后更新文档

请将现在的任务管理和节点管理流程，具体到每一个调用的方法，整理出来，看看有没有重复的调用或者错误的调用会导致不必要的开销，然后给我一个文档交给决策部门审议

这个项目的开发力量不足，无法对每个模块每个流程都做到滚瓜烂熟。我希望代码逻辑尽可能简单易懂，方便找到问题，而不是添加一层又一层的保险措施来掩盖问题

请把web端对音频的接收和发送、播放按钮的前后流程，具体到每个调用方法，整理成文档，让我交给决策部门

现在请将ASR模块的流程和代码逻辑，具体到每个方法的调用，整理成文档，让我交给决策部门审议。请确认代码逻辑没有重复或者矛盾

这是本次改造之后的的调度服务器任务管理流程，请看一下有没有重复或者矛盾的逻辑，有没有遗漏或者可以优化的地方




现在我们开始进行一次语音识别稳定性测试。
我会先读一两句比较短的话，用来确认系统不会在句子之间随意地把语音切断，或者在没有必要的时候提前结束本次识别。

接下来这一句我会尽量连续地说得长一些，中间只保留自然的呼吸节奏，不做刻意的停顿，看看在超过十秒钟之后，系统会不会因为超时或者静音判定而强行把这句话截断，从而导致前半句和后半句在节点端被拆成两个不同的 job，甚至出现语义上不完整、读起来前后不连贯的情况。

如果这次的长句能够被完整地识别出来，而且不会出现半句话被提前发送或者直接丢失的现象，那就说明我们当前的切分策略和超时规则是基本可用的。
否则，我们还需要继续分析日志，找出到底是在哪一个环节把我的语音吃掉了。



我想对超长语音做出更好的处理，以提升用户体验：假设场景是35秒的长语音，被调度服务器拆成4个job，进入节点端后被AudioAggregator切分成多个片段：job0_1:3秒，job0_2:3秒，job0_3:4秒，job1_1:3秒，job1_2:3秒，job1_3:4秒，job2_1:3秒，job2_2:3秒，job2_3:3秒，job2_4:1秒，job3_1:5秒。这时虽然AudioAggregator会一直等到job3带着手动截断或者pause截断的标识来了才算完成合并，但在这之前，这些片段已经被分别合并送入ASR了，其中job0_1+job0_2=6秒，job0_3+job1_1=7秒，job1_2+job1_3=7秒，job2_1+job2_2=6秒，job2_3+job2_4+job3_1=9秒还是会出现5个语音识别文本的切片，如果utterance拼接需要等到这个5个切片都识别完成再发给语义修复服务，也会有一个很长的延迟，再加上语义修复和NMT和TTS处理的时间，用户可能需要等30秒以上才有结果，这就违背了流式处理的初衷。所以我希望在utterance中，将语音识别文本与job从头部对齐，也就是job0_1+job0_2=6秒作为job0的结果，job0_3+job1_1=7秒和job1_2+job1_3=7秒合并作为job1的结果，job2_1+job2_2=6秒和job2_3+job2_4+job3_1=9秒作为job2的结果进行下一步处理，发送给语义修复/NMT/TTS，返回web端，如果没有语音识别文本是以job3开头的，就把job3视为合并入job2了。也就是audioAggregator会将Job按能量切分，但utterance会将语音识别文本合并，只要第一个文本片段属于哪个job，就将这个文本片段作为该job的结果返回，哪怕这个文本片段是由多个超过5秒语音片段组成的。这样做的目的是确保切片数量不会超过job容器数量，不会产生文本丢失的情况

# 运行所有测试（不包括ignore的）
cargo test

# 运行包括ignore的测试（需要Redis）
cargo test -- --ignored

# 只运行Session Affinity测试
cargo test session_affinity