# 决策报告：中文语义修复服务技术方案变更

## 文档信息
- **报告日期**：2026-01-02
- **服务名称**：Semantic Repair ZH（中文语义修复服务）
- **报告类型**：技术方案变更决策报告
- **目标受众**：决策部门

---

## 执行摘要

中文语义修复服务在实施过程中遇到 GPTQ 量化模型加载的兼容性问题。经过技术评估和验证，决定采用 **llama.cpp（GGUF 格式）** 作为唯一的推理引擎，替代原有的 auto-gptq + transformers 方案。该方案已实施完成，代码已清理，等待测试验证。

---

## 一、问题背景

### 1.1 原始技术方案
- **模型格式**：GPTQ INT4 量化模型
- **加载库**：auto-gptq + transformers
- **依赖**：PyTorch 2.5.1+cu121

### 1.2 遇到的问题
1. **auto-gptq 兼容性问题**
   - auto-gptq 0.7.1 与 PyTorch 2.5.1 存在兼容性问题
   - 模型加载时出现 "Cannot copy out of meta tensor" 错误
   - 尝试多种加载策略均失败

2. **依赖冲突**
   - 降级 auto-gptq 导致 NumPy 版本冲突
   - 降级 PyTorch 会影响其他服务（NMT、TTS、Speaker Embedding）

3. **编译问题**
   - llama-cpp-python 在 Windows 上编译困难
   - 最终通过源码编译成功安装

---

## 二、技术决策

### 2.1 决策：采用 llama.cpp（GGUF）作为唯一引擎

**决策时间**：2026-01-02

**决策理由**：
1. ✅ **不影响其他服务**：保持 PyTorch 2.5.1+cu121，其他服务已验证可正常使用 CUDA
2. ✅ **完全避免兼容性问题**：不依赖 auto-gptq，使用独立的 llama.cpp 库
3. ✅ **符合架构设计**：与 `SEMANTIC_REPAIR_DUAL_ENGINE_AUTOTUNE_SPEC.md` 文档建议一致
4. ✅ **资源占用低**：GGUF 4bit 量化模型常驻资源低，适合节点端场景
5. ✅ **避免依赖冲突**：独立的依赖树，不与其他服务冲突

### 2.2 放弃的方案

#### 方案A：降级 PyTorch（已放弃）
- **原因**：会影响其他服务（NMT、TTS、Speaker Embedding）的 CUDA 支持
- **风险**：需要回滚，影响面大

#### 方案B：修复 auto-gptq（已放弃）
- **原因**：兼容性问题复杂，等待上游修复时间不确定
- **风险**：项目进度受阻

#### 方案C：使用 ExLlamaV2（未实施）
- **原因**：优先尝试 llama.cpp，如果失败可考虑此方案
- **状态**：保留为备选方案

---

## 三、实施情况

### 3.1 已完成的工作

#### 3.1.1 依赖安装
- ✅ `llama-cpp-python` 已成功安装（从源码编译）
- ✅ 验证导入成功

#### 3.1.2 模型下载
- ✅ GGUF 模型已下载到 `models/qwen2.5-3b-instruct-zh-gguf/`
- ✅ 可用模型：
  - `qwen2.5-3b-instruct-q4_k_m.gguf` (1.96 GB) - 推荐使用
  - `qwen2.5-3b-instruct-q4_0.gguf` (1.86 GB)
  - 其他量化版本

#### 3.1.3 代码实现
- ✅ 创建 `llamacpp_engine.py` - Llama.cpp 引擎适配器
- ✅ 实现统一的 `repair()` 接口
- ✅ 修改服务启动逻辑，只使用 llama.cpp
- ✅ 移除所有回退逻辑和自动选择逻辑
- ✅ 更新 `/repair`, `/health`, `/diagnostics` 端点

#### 3.1.4 代码清理
- ✅ 移除 transformers/auto-gptq 相关代码
- ✅ 移除 bitsandbytes 量化相关代码
- ✅ 移除 RepairEngine 相关代码
- ✅ 简化全局变量（只保留 `llamacpp_engine`）

### 3.2 当前状态

**代码状态**：✅ 已完成，无 linter 错误

**待测试**：
- [ ] 服务启动测试
- [ ] 修复功能测试
- [ ] 性能测试
- [ ] 内存使用测试

---

## 四、影响分析

### 4.1 对其他服务的影响

**验证结果**：✅ 无影响

已验证以下服务在 PyTorch 2.5.1+cu121 下可正常使用 CUDA：
- ✅ NMT 服务 (nmt_m2m100)
- ✅ Semantic Repair EN 服务
- ✅ Speaker Embedding 服务

**结论**：不降级 PyTorch 的决策是正确的，其他服务不受影响。

### 4.2 对当前服务的影响

#### 优点
1. ✅ 完全避免 auto-gptq 兼容性问题
2. ✅ 独立的依赖树，不与其他服务冲突
3. ✅ 代码更简洁，维护成本更低
4. ✅ 符合架构设计文档建议

#### 风险
1. ⚠️ 如果 llama.cpp 引擎加载失败，服务无法启动（不回退）
2. ⚠️ 需要确保 GGUF 模型文件完整
3. ⚠️ 首次使用需要测试验证

### 4.3 资源占用

**预期**：
- 模型大小：~2 GB（GGUF 4bit）
- 内存占用：预计低于 transformers 方案
- GPU 占用：支持 GPU 加速（n_gpu_layers=-1）

**待验证**：实际测试后更新

---

## 五、技术规格

### 5.1 模型信息
- **模型名称**：Qwen2.5-3B-Instruct-GGUF
- **量化格式**：GGUF 4bit（q4_k_m 或 q4_0）
- **模型大小**：~2 GB
- **存储位置**：`models/qwen2.5-3b-instruct-zh-gguf/`

### 5.2 引擎配置
- **引擎**：llama.cpp（通过 llama-cpp-python）
- **上下文窗口**：2048 tokens
- **GPU 层数**：-1（使用所有 GPU 层）
- **生成参数**：
  - max_tokens: 64
  - temperature: 0.1
  - top_p: 0.9
  - top_k: 40

### 5.3 依赖要求
- **必需**：
  - `llama-cpp-python` >= 0.3.16
  - `fastapi` >= 0.104.0
  - `uvicorn` >= 0.24.0
- **可选**：
  - CUDA（用于 GPU 加速）
  - PyTorch（仅用于 GPU 信息检查，不用于模型推理）

---

## 六、下一步行动

### 6.1 立即行动（P0）
1. **测试服务启动**
   - 验证 GGUF 模型加载
   - 检查错误处理和日志

2. **功能测试**
   - 测试 `/repair` 端点
   - 验证修复质量
   - 检查响应时间

3. **性能测试**
   - 内存使用情况
   - GPU 使用情况
   - 响应延迟

### 6.2 后续优化（P1）
1. 完善错误处理和日志
2. 优化推理性能
3. 添加资源监控

### 6.3 长期规划（P2）
1. 实现双引擎自动切换（按文档）
2. 添加基准测试和运行时监控
3. 实现引擎选择策略

---

## 七、风险评估

### 7.1 技术风险

| 风险 | 影响 | 概率 | 缓解措施 |
|------|------|------|----------|
| llama.cpp 引擎加载失败 | 高 | 低 | 已实现错误处理，会显示清晰的错误信息 |
| GGUF 模型文件不完整 | 高 | 低 | 启动时检查模型文件，缺失时直接失败 |
| 性能不达标 | 中 | 中 | 保留 ExLlamaV2 作为备选方案 |
| GPU 加速不可用 | 中 | 低 | 支持 CPU fallback（需要测试） |

### 7.2 项目风险

| 风险 | 影响 | 概率 | 缓解措施 |
|------|------|------|----------|
| 测试发现问题 | 中 | 中 | 已准备回滚方案（ExLlamaV2） |
| 文档不完整 | 低 | 低 | 已更新相关文档 |

---

## 八、结论

经过技术评估和验证，采用 **llama.cpp（GGUF）作为唯一推理引擎** 是当前最优方案：

1. ✅ **解决了兼容性问题**：完全避免 auto-gptq 的兼容性问题
2. ✅ **不影响其他服务**：保持 PyTorch 版本，其他服务正常运行
3. ✅ **符合架构设计**：与文档建议一致
4. ✅ **代码已实施完成**：等待测试验证

**建议**：批准当前方案，进行测试验证。如果测试发现问题，可考虑 ExLlamaV2 作为备选方案。

---

## 附录

### A. 相关文档
- [当前问题与解决方案](CURRENT_ISSUE_AND_SOLUTION.md)
- [实施决策记录](IMPLEMENTATION_DECISION.md)
- [服务兼容性验证](SERVICE_COMPATIBILITY_VERIFICATION.md)
- [Llama.cpp 实施状态](LLAMACPP_IMPLEMENTATION_STATUS.md)
- [代码清理总结](CLEANUP_SUMMARY.md)

### B. 技术参考
- [双引擎自适应方案](../SEMANTIC_REPAIR_DUAL_ENGINE_AUTOTUNE_SPEC.md)
- [GPU 仲裁 MVP 技术方案](../../../docs/GPU/GPU_ARBITRATION_MVP_TECH_SPEC.md)

### C. 联系方式
如有疑问，请联系技术团队。

---

**报告结束**
