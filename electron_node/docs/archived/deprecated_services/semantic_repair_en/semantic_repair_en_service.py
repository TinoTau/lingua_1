# -*- coding: utf-8 -*-
"""
Semantic Repair Service - English
è‹±æ–‡è¯­ä¹‰ä¿®å¤æœåŠ¡ä¸»æ–‡ä»¶
"""

import sys
import io
import os
import time
import signal
import traceback
import logging
from typing import Optional, Dict, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import torch
import gc  # For garbage collection

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [Semantic Repair EN] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

from model_loader import (
    setup_device,
    log_gpu_info,
    find_gguf_model_path,
)
from llamacpp_engine import LlamaCppEngine

# å¼ºåˆ¶è®¾ç½®æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºä¸º UTF-8 ç¼–ç ï¼ˆWindows å…¼å®¹æ€§ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(
        sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=False
    )
    sys.stderr = io.TextIOWrapper(
        sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=False
    )

# å…¨å±€å¼‚å¸¸å¤„ç†ï¼ˆæ•è·æœªå¤„ç†çš„å¼‚å¸¸ï¼Œé˜²æ­¢æœåŠ¡å´©æºƒï¼‰
def handle_exception(exc_type, exc_value, exc_traceback):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    
    print("=" * 80, flush=True)
    print(f"[Semantic Repair EN] ğŸš¨ Uncaught exception in main process, service may crash", flush=True)
    print(f"[Semantic Repair EN] Exception type: {exc_type.__name__}", flush=True)
    print(f"[Semantic Repair EN] Exception value: {exc_value}", flush=True)
    print("[Semantic Repair EN] Traceback:", flush=True)
    for line in traceback.format_exception(exc_type, exc_value, exc_traceback):
        print(f"[Semantic Repair EN] {line.rstrip()}", flush=True)
    print("=" * 80, flush=True)
    
    # è°ƒç”¨é»˜è®¤å¼‚å¸¸å¤„ç†å™¨
    sys.__excepthook__(exc_type, exc_value, exc_traceback)

sys.excepthook = handle_exception

# ä¿¡å·å¤„ç†ï¼ˆç”¨äºè®°å½•ä¸»è¿›ç¨‹é€€å‡ºï¼‰
def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    print(f"[Semantic Repair EN] Received signal {signum}, preparing to shutdown...", flush=True)
    if signum == signal.SIGTERM:
        print("[Semantic Repair EN] SIGTERM received, graceful shutdown", flush=True)
    elif signum == signal.SIGINT:
        print("[Semantic Repair EN] SIGINT received (Ctrl+C), graceful shutdown", flush=True)
    else:
        print(f"[Semantic Repair EN] Unexpected signal {signum} received", flush=True)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆWindows ä¸Šå¯èƒ½ä¸æ”¯æŒæ‰€æœ‰ä¿¡å·ï¼‰
try:
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
except (ValueError, OSError) as e:
    # Windows ä¸Šå¯èƒ½ä¸æ”¯æŒæŸäº›ä¿¡å·
    print(f"[Semantic Repair EN] Warning: Could not register signal handler: {e}", flush=True)

# å…¨å±€å˜é‡ï¼ˆå°†åœ¨startupæ—¶é€šè¿‡setup_device()è®¾ç½®ï¼‰
DEVICE = None  # å°†åœ¨startupæ—¶åˆå§‹åŒ–
llamacpp_engine: Optional[LlamaCppEngine] = None  # llama.cpp å¼•æ“
loaded_model_path: Optional[str] = None
model_warmed = False


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆå¯åŠ¨å’Œä¼˜é›…å…³é—­ï¼‰"""
    global llamacpp_engine, loaded_model_path, DEVICE, model_warmed
    
    startup_start_time = time.time()
    
    # ==================== å¯åŠ¨æ—¶æ‰§è¡Œ ====================
    try:
        print("[Semantic Repair EN] ===== Starting Semantic Repair Service (English) =====", flush=True)
        print(f"[Semantic Repair EN] Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
        print(f"[Semantic Repair EN] Python version: {sys.version}", flush=True)
        print(f"[Semantic Repair EN] PyTorch version: {torch.__version__}", flush=True)
        print(f"[Semantic Repair EN] CUDA available: {torch.cuda.is_available()}", flush=True)
        print("[Semantic Repair EN] âš ï¸  Model loading may cause high CPU usage, please wait...", flush=True)
        
        # è®¾ç½®è®¾å¤‡ï¼ˆå¼ºåˆ¶GPUï¼Œå¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        step_start = time.time()
        print("[Semantic Repair EN] [1/5] Setting up device...", flush=True)
        DEVICE = setup_device()
        print(f"[Semantic Repair EN] Device: {DEVICE} (took {time.time() - step_start:.2f}s)", flush=True)
        
        # è®°å½•GPUä¿¡æ¯
        log_gpu_info()
        
        # å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        os.environ["HF_HUB_DISABLE_IMPLICIT_TOKEN"] = "1"
        os.environ["HF_LOCAL_FILES_ONLY"] = "1"
        
        # ä»æœåŠ¡ç›®å½•æŸ¥æ‰¾ GGUF æ¨¡å‹
        step_start = time.time()
        print("[Semantic Repair EN] [2/5] Finding GGUF model path...", flush=True)
        service_dir = os.path.dirname(__file__)
        
        gguf_model_path = find_gguf_model_path(service_dir)
        if not gguf_model_path:
            error_msg = (
                f"[Semantic Repair EN] âŒ ERROR: GGUF model not found!\n"
                f"  Service directory: {service_dir}\n"
                f"  Expected location: {os.path.join(service_dir, 'models', 'qwen2.5-3b-instruct-en-gguf')}\n"
                f"  \n"
                f"  Please download GGUF model files:\n"
                f"    hf download Qwen/Qwen2.5-3B-Instruct-GGUF --include \"*.gguf\" --local-dir ./models/qwen2.5-3b-instruct-en-gguf\n"
                f"  \n"
                f"  Or use the Chinese model (supports English):\n"
                f"    The Chinese model at models/qwen2.5-3b-instruct-zh-gguf can also be used for English.\n"
                f"  \n"
                f"  Service startup is ABORTED."
            )
            print(error_msg, flush=True)
            raise FileNotFoundError(
                f"GGUF model not found. Please download the model to: "
                f"{os.path.join(service_dir, 'models', 'qwen2.5-3b-instruct-en-gguf')}"
            )
        
        print(f"[Semantic Repair EN] Found GGUF model: {gguf_model_path}", flush=True)
        
        # åŠ è½½ llama.cpp å¼•æ“
        step_start = time.time()
        print("[Semantic Repair EN] [3/5] Loading llama.cpp engine...", flush=True)
        llamacpp_engine = LlamaCppEngine(
            model_path=gguf_model_path,
            n_ctx=2048,
            n_gpu_layers=-1,  # ä½¿ç”¨æ‰€æœ‰ GPU å±‚
            verbose=False
        )
        loaded_model_path = gguf_model_path
        model_load_time = time.time() - step_start
        print(f"[Semantic Repair EN] llama.cpp engine loaded (took {model_load_time:.2f}s)", flush=True)
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        print("[Semantic Repair EN] Memory cleanup completed", flush=True)
        
        print("[Semantic Repair EN] âœ… Llama.cpp engine initialized successfully", flush=True)
        
        # æ¨¡å‹é¢„çƒ­ï¼ˆåœ¨å¯åŠ¨æ—¶è¿›è¡Œï¼Œè€Œä¸æ˜¯ç­‰åˆ°ç¬¬ä¸€æ¬¡APIè°ƒç”¨ï¼‰
        step_start = time.time()
        print("[Semantic Repair EN] [4/5] Warming up llama.cpp engine (this may take 10-30 seconds)...", flush=True)
        print(f"[Semantic Repair EN] Warm-up started at {time.strftime('%H:%M:%S')}", flush=True)
        try:
            warmup_text = "Hello, this is a test sentence."
            _ = llamacpp_engine.repair(warmup_text)
            model_warmed = True
            warmup_time = time.time() - step_start
            print(f"[Semantic Repair EN] âœ… Model warm-up completed (took {warmup_time:.2f}s)", flush=True)
        except Exception as e:
            print(f"[Semantic Repair EN] âš ï¸  Warm-up failed (will warm-up on first request): {e}", flush=True)
            model_warmed = False
        
        total_startup_time = time.time() - startup_start_time
        print(f"[Semantic Repair EN] âœ… Service is ready (total startup time: {total_startup_time:.2f}s)", flush=True)
    except Exception as e:
        print(f"[Semantic Repair EN] [CRITICAL ERROR] Failed to initialize: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise
    
    yield  # åº”ç”¨è¿è¡ŒæœŸé—´
    
    yield  # åº”ç”¨è¿è¡ŒæœŸé—´
    
    # ==================== å…³é—­æ—¶æ‰§è¡Œï¼ˆä¼˜é›…å…³é—­ï¼‰ ====================
    try:
        print("[Semantic Repair EN] ===== Shutting down Semantic Repair Service (English) =====", flush=True)
        print(f"[Semantic Repair EN] Main process PID: {os.getpid()}", flush=True)
        
        # æ¸…ç† llama.cpp å¼•æ“
        if llamacpp_engine is not None:
            print("[Semantic Repair EN] Cleaning up llama.cpp engine...", flush=True)
            llamacpp_engine.shutdown()
            llamacpp_engine = None
        
        # æ¸…ç†GPUç¼“å­˜
        if DEVICE is not None and DEVICE.type == "cuda":
            print("[Semantic Repair EN] Clearing GPU cache...", flush=True)
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        print("[Semantic Repair EN] âœ… Graceful shutdown completed", flush=True)
    except Exception as e:
        print(f"[Semantic Repair EN] âŒ Error during shutdown: {e}", flush=True)
        import traceback
        traceback.print_exc()


# åˆ›å»º FastAPI åº”ç”¨ï¼ˆä½¿ç”¨lifespanæ›¿ä»£@app.on_eventï¼‰
app = FastAPI(
    title="Semantic Repair Service - English",
    version="1.0.0",
    lifespan=lifespan
)


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class RepairRequest(BaseModel):
    """ä¿®å¤è¯·æ±‚"""
    job_id: str
    session_id: str
    utterance_index: int = 0
    lang: str = Field(default="en", description="è¯­è¨€ä»£ç ")
    text_in: str = Field(..., description="è¾“å…¥æ–‡æœ¬")
    quality_score: Optional[float] = Field(default=None, description="è´¨é‡åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰")
    micro_context: Optional[str] = Field(default=None, description="å¾®ä¸Šä¸‹æ–‡ï¼ˆä¸Šä¸€å¥å°¾éƒ¨ï¼‰")
    meta: Optional[Dict] = Field(default=None, description="å…ƒæ•°æ®")


class RepairResponse(BaseModel):
    """ä¿®å¤å“åº”"""
    decision: str = Field(..., description="å†³ç­–ï¼šPASSã€REPAIR æˆ– REJECT")
    text_out: str = Field(..., description="è¾“å‡ºæ–‡æœ¬")
    confidence: float = Field(..., description="ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰")
    diff: List[Dict] = Field(default_factory=list, description="å·®å¼‚åˆ—è¡¨")
    reason_codes: List[str] = Field(default_factory=list, description="åŸå› ä»£ç åˆ—è¡¨")
    repair_time_ms: Optional[int] = Field(default=None, description="ä¿®å¤è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰")


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field(..., description="çŠ¶æ€ï¼šhealthyã€loading æˆ– error")
    model_loaded: bool = Field(..., description="æ¨¡å‹æ˜¯å¦å·²åŠ è½½")
    model_version: Optional[str] = Field(default=None, description="æ¨¡å‹ç‰ˆæœ¬")
    warmed: bool = Field(default=False, description="æ¨¡å‹æ˜¯å¦å·²warm")


# ==================== API ç«¯ç‚¹ ====================
# æ³¨æ„ï¼šå¯åŠ¨å’Œå…³é—­é€»è¾‘å·²ç§»è‡³ lifespan ä¸Šä¸‹æ–‡ç®¡ç†å™¨


# ==================== API ç«¯ç‚¹ ====================

@app.post("/repair", response_model=RepairResponse)
async def repair_text(request: RepairRequest):
    """
    ä¿®å¤ASRæ–‡æœ¬
    
    å¯¹ASRè¾“å‡ºçš„è‹±æ–‡æ–‡æœ¬è¿›è¡Œè¯­ä¹‰ä¿®å¤ï¼Œä¸»è¦è§£å†³æ‹¼å†™é”™è¯¯ã€ç¼©å†™è¯¯è¯†åˆ«ç­‰é—®é¢˜ã€‚
    """
    global llamacpp_engine, model_warmed
    
    # æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨
    if llamacpp_engine is None:
        raise HTTPException(status_code=503, detail="Llama.cpp engine not initialized")
    
    # åªå¤„ç†è‹±æ–‡
    if request.lang != "en":
        return RepairResponse(
            decision="PASS",
            text_out=request.text_in,
            confidence=1.0,
            reason_codes=["NOT_ENGLISH"],
        )
    
    # æ¨¡å‹é¢„çƒ­ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶ï¼‰
    if not model_warmed:
        try:
            # llama.cpp ä¸éœ€è¦å•ç‹¬çš„ warm_upï¼Œç›´æ¥ä½¿ç”¨å³å¯
            model_warmed = True
        except Exception as e:
            print(f"[Semantic Repair EN] Warm-up failed: {e}", flush=True)
    
    start_time = time.time()
    
    # è®°å½•è¾“å…¥ï¼ˆä»»åŠ¡é“¾æ—¥å¿—ï¼‰- åŒæ—¶ä½¿ç”¨printç¡®ä¿è¾“å‡ºå¯è§
    input_log = (
        f"SEMANTIC_REPAIR_EN INPUT: Received repair request | "
        f"job_id={request.job_id} | "
        f"session_id={request.session_id} | "
        f"utterance_index={request.utterance_index} | "
        f"lang={request.lang} | "
        f"text_in={request.text_in!r} | "
        f"text_in_length={len(request.text_in)} | "
        f"quality_score={request.quality_score} | "
        f"micro_context={repr(request.micro_context) if request.micro_context else None}"
    )
    logger.info(input_log)
    print(f"[Semantic Repair EN] {input_log}", flush=True)
    
    try:
        # æ‰§è¡Œä¿®å¤
        result = llamacpp_engine.repair(
            text_in=request.text_in,
            micro_context=request.micro_context,
            quality_score=request.quality_score
        )
        
        elapsed_ms = int((time.time() - start_time) * 1000)
        
        # æ„å»ºå“åº”
        decision = "REPAIR" if result['text_out'] != request.text_in else "PASS"
        reason_codes = []
        
        # é™ä½è´¨é‡åˆ†æ•°é˜ˆå€¼ï¼Œæé«˜æ•æ„Ÿåº¦ï¼šä»0.7é™åˆ°0.85ï¼ˆä¸ä¸­æ–‡æœåŠ¡ä¿æŒä¸€è‡´ï¼‰
        if request.quality_score is not None and request.quality_score < 0.85:
            reason_codes.append("LOW_QUALITY_SCORE")
        
        if decision == "REPAIR":
            reason_codes.append("REPAIR_APPLIED")
        
        # è®°å½•è¾“å‡ºï¼ˆä»»åŠ¡é“¾æ—¥å¿—ï¼‰- åŒæ—¶ä½¿ç”¨printç¡®ä¿è¾“å‡ºå¯è§
        output_log = (
            f"SEMANTIC_REPAIR_EN OUTPUT: Repair completed | "
            f"job_id={request.job_id} | "
            f"session_id={request.session_id} | "
            f"utterance_index={request.utterance_index} | "
            f"decision={decision} | "
            f"text_out={result['text_out']!r} | "
            f"text_out_length={len(result['text_out'])} | "
            f"confidence={result['confidence']:.2f} | "
            f"reason_codes={reason_codes} | "
            f"repair_time_ms={elapsed_ms} | "
            f"changed={result['text_out'] != request.text_in}"
        )
        logger.info(output_log)
        print(f"[Semantic Repair EN] {output_log}", flush=True)
        
        return RepairResponse(
            decision=decision,
            text_out=result['text_out'],
            confidence=result['confidence'],
            diff=result['diff'],
            reason_codes=reason_codes,
            repair_time_ms=elapsed_ms,
        )
    except Exception as e:
        print(f"[Semantic Repair EN] Error during repair: {e}", flush=True)
        import traceback
        traceback.print_exc()
        
        # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›åŸæ–‡
        return RepairResponse(
            decision="PASS",
            text_out=request.text_in,
            confidence=0.5,
            reason_codes=["ERROR"],
            repair_time_ms=int((time.time() - start_time) * 1000),
        )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    è¿”å›æœåŠ¡å¥åº·çŠ¶æ€å’Œæ¨¡å‹warmçŠ¶æ€
    åªæœ‰åœ¨warm-upå®Œæˆåæ‰è¿”å›"healthy"çŠ¶æ€
    """
    global llamacpp_engine, loaded_model_path, model_warmed
    
    if llamacpp_engine is None:
        return HealthResponse(
            status="loading",  # å¼•æ“æ­£åœ¨åŠ è½½ä¸­
            model_loaded=False,
            warmed=False,
        )
    
    # å¦‚æœå¼•æ“å·²åŠ è½½ä½†æœªå®Œæˆwarm-upï¼Œè¿”å›"loading"çŠ¶æ€
    if not model_warmed:
        return HealthResponse(
            status="loading",  # å¼•æ“å·²åŠ è½½ï¼Œä½†æ­£åœ¨warm-upä¸­
            model_loaded=True,
            warmed=False,
        )
    
    model_version = "qwen2.5-3b-instruct-en-gguf"
    if loaded_model_path:
        # ä»è·¯å¾„æå–æ¨¡å‹ç‰ˆæœ¬
        model_name = os.path.basename(loaded_model_path)
        if model_name:
            model_version = model_name
    
    # åªæœ‰åœ¨warm-upå®Œæˆåæ‰è¿”å›"healthy"
    return HealthResponse(
        status="healthy",
        model_loaded=True,
        model_version=model_version,
        warmed=model_warmed,
    )


# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    import uvicorn
    
    # ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–ç«¯å£
    port = int(os.environ.get("PORT", 5011))
    host = os.environ.get("HOST", "127.0.0.1")
    
    print(f"[Semantic Repair EN] Starting server on {host}:{port}", flush=True)
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        workers=1,  # å•è¿›ç¨‹ï¼Œé¿å…å¤šè¿›ç¨‹å¯¼è‡´çš„é«˜CPUå ç”¨
        loop="asyncio",  # ä½¿ç”¨asyncioäº‹ä»¶å¾ªç¯
    )
