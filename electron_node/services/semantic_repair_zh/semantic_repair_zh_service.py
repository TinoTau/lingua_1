# -*- coding: utf-8 -*-
"""
Semantic Repair Service - Chinese
ä¸­æ–‡è¯­ä¹‰ä¿®å¤æœåŠ¡ä¸»æ–‡ä»¶
"""

import sys
import io
import os
import time
import traceback  # For global exception handling
import signal  # For signal handling
import logging
from typing import Optional, Dict, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import torch
import gc  # For garbage collection

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [Semantic Repair ZH] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

from model_loader import (
    setup_device,
    log_gpu_info,
    find_gguf_model_path,
)
from llamacpp_engine import LlamaCppEngine

# å¼ºåˆ¶è®¾ç½®æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºä¸º UTF-8 ç¼–ç ï¼ˆWindows å…¼å®¹æ€§ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(
        sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=False
    )
    sys.stderr = io.TextIOWrapper(
        sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=False
    )

# å…¨å±€å¼‚å¸¸å¤„ç†ï¼ˆæ•è·æœªå¤„ç†çš„å¼‚å¸¸ï¼Œé˜²æ­¢æœåŠ¡å´©æºƒï¼‰
def handle_exception(exc_type, exc_value, exc_traceback):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    
    print("=" * 80, flush=True)
    print(f"[Semantic Repair ZH] ğŸš¨ Uncaught exception in main process, service may crash", flush=True)
    print(f"[Semantic Repair ZH] Exception type: {exc_type.__name__}", flush=True)
    print(f"[Semantic Repair ZH] Exception value: {exc_value}", flush=True)
    print("[Semantic Repair ZH] Traceback:", flush=True)
    for line in traceback.format_exception(exc_type, exc_value, exc_traceback):
        print(f"[Semantic Repair ZH] {line.rstrip()}", flush=True)
    print("=" * 80, flush=True)
    
    # è°ƒç”¨é»˜è®¤å¼‚å¸¸å¤„ç†å™¨
    sys.__excepthook__(exc_type, exc_value, exc_traceback)

sys.excepthook = handle_exception

# ä¿¡å·å¤„ç†ï¼ˆç”¨äºè®°å½•ä¸»è¿›ç¨‹é€€å‡ºï¼‰
def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    print(f"[Semantic Repair ZH] Received signal {signum}, preparing to shutdown...", flush=True)
    if signum == signal.SIGTERM:
        print("[Semantic Repair ZH] SIGTERM received, graceful shutdown", flush=True)
    elif signum == signal.SIGINT:
        print("[Semantic Repair ZH] SIGINT received (Ctrl+C), graceful shutdown", flush=True)
    else:
        print(f"[Semantic Repair ZH] Unexpected signal {signum} received", flush=True)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆWindows ä¸Šå¯èƒ½ä¸æ”¯æŒæ‰€æœ‰ä¿¡å·ï¼‰
try:
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
except (ValueError, OSError) as e:
    # Windows ä¸Šå¯èƒ½ä¸æ”¯æŒæŸäº›ä¿¡å·
    print(f"[Semantic Repair ZH] Warning: Could not register signal handler: {e}", flush=True)

# å…¨å±€å˜é‡ï¼ˆå°†åœ¨startupæ—¶é€šè¿‡setup_device()è®¾ç½®ï¼‰
DEVICE = None  # å°†åœ¨startupæ—¶åˆå§‹åŒ–
llamacpp_engine: Optional[LlamaCppEngine] = None  # llama.cpp å¼•æ“
loaded_model_path: Optional[str] = None
model_warmed = False


def log_resource_usage(stage: str, device=None):
    """è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ"""
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        cpu_percent = process.cpu_percent(interval=0.1)
        
        device_to_check = device if device is not None else DEVICE
        if device_to_check is not None and device_to_check.type == "cuda" and torch.cuda.is_available():
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"[Semantic Repair ZH] [{stage}] Memory: {memory_mb:.2f} MB | CPU: {cpu_percent:.1f}% | GPU Allocated: {gpu_memory_allocated:.2f} GB | GPU Reserved: {gpu_memory_reserved:.2f} GB", flush=True)
        else:
            print(f"[Semantic Repair ZH] [{stage}] Memory: {memory_mb:.2f} MB | CPU: {cpu_percent:.1f}%", flush=True)
    except Exception as e:
        print(f"[Semantic Repair ZH] [{stage}] Failed to log resource usage: {e}", flush=True)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆå¯åŠ¨å’Œä¼˜é›…å…³é—­ï¼‰"""
    global llamacpp_engine, loaded_model_path, DEVICE, model_warmed
    
    startup_start_time = time.time()
    
    # ==================== å¯åŠ¨æ—¶æ‰§è¡Œ ====================
    try:
        print("[Semantic Repair ZH] ===== Starting Semantic Repair Service (Chinese) =====", flush=True)
        print(f"[Semantic Repair ZH] Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
        print(f"[Semantic Repair ZH] Python version: {sys.version}", flush=True)
        print(f"[Semantic Repair ZH] PyTorch version: {torch.__version__}", flush=True)
        print(f"[Semantic Repair ZH] CUDA available: {torch.cuda.is_available()}", flush=True)
        print("[Semantic Repair ZH] âš ï¸  Model loading may cause high CPU usage, please wait...", flush=True)
        log_resource_usage("INIT")
        
        # è®¾ç½®è®¾å¤‡ï¼ˆå¼ºåˆ¶GPUï¼Œå¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        step_start = time.time()
        print("[Semantic Repair ZH] [1/5] Setting up device...", flush=True)
        DEVICE = setup_device()
        print(f"[Semantic Repair ZH] Device: {DEVICE} (took {time.time() - step_start:.2f}s)", flush=True)
        log_resource_usage("DEVICE_SETUP")
        
        # è®°å½•GPUä¿¡æ¯
        log_gpu_info()
        
        # å¼ºåˆ¶åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        os.environ["HF_HUB_DISABLE_IMPLICIT_TOKEN"] = "1"
        os.environ["HF_LOCAL_FILES_ONLY"] = "1"
        
        # ä»æœåŠ¡ç›®å½•æŸ¥æ‰¾ GGUF æ¨¡å‹
        step_start = time.time()
        print("[Semantic Repair ZH] [2/5] Finding GGUF model path...", flush=True)
        service_dir = os.path.dirname(__file__)
        
        gguf_model_path = find_gguf_model_path(service_dir)
        if not gguf_model_path:
            error_msg = (
                f"[Semantic Repair ZH] âŒ ERROR: GGUF model not found!\n"
                f"  Service directory: {service_dir}\n"
                f"  Expected location: {os.path.join(service_dir, 'models', 'qwen2.5-3b-instruct-zh-gguf')}\n"
                f"  \n"
                f"  Please download GGUF model files:\n"
                f"    hf download Qwen/Qwen2.5-3B-Instruct-GGUF --include \"*.gguf\" --local-dir ./models/qwen2.5-3b-instruct-zh-gguf\n"
                f"  \n"
                f"  Service startup is ABORTED."
            )
            print(error_msg, flush=True)
            raise FileNotFoundError(
                f"GGUF model not found. Please download the model to: "
                f"{os.path.join(service_dir, 'models', 'qwen2.5-3b-instruct-zh-gguf')}"
            )
        
        print(f"[Semantic Repair ZH] Found GGUF model: {gguf_model_path}", flush=True)
        
        # åŠ è½½ llama.cpp å¼•æ“
        step_start = time.time()
        print("[Semantic Repair ZH] [3/5] Loading llama.cpp engine...", flush=True)
        llamacpp_engine = LlamaCppEngine(
            model_path=gguf_model_path,
            n_ctx=2048,
            n_gpu_layers=-1,  # ä½¿ç”¨æ‰€æœ‰ GPU å±‚
            verbose=False
        )
        loaded_model_path = gguf_model_path
        model_load_time = time.time() - step_start
        print(f"[Semantic Repair ZH] llama.cpp engine loaded (took {model_load_time:.2f}s)", flush=True)
        log_resource_usage("LLAMACPP_ENGINE_LOADED")
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        print("[Semantic Repair ZH] Memory cleanup completed", flush=True)
        log_resource_usage("MEMORY_CLEANUP")
        
        print("[Semantic Repair ZH] âœ… Llama.cpp engine initialized successfully", flush=True)
        
        # æ¨¡å‹é¢„çƒ­ï¼ˆåœ¨å¯åŠ¨æ—¶è¿›è¡Œï¼Œè€Œä¸æ˜¯ç­‰åˆ°ç¬¬ä¸€æ¬¡APIè°ƒç”¨ï¼‰
        step_start = time.time()
        print("[Semantic Repair ZH] [4/5] Warming up llama.cpp engine (this may take 10-30 seconds)...", flush=True)
        print(f"[Semantic Repair ZH] Warm-up started at {time.strftime('%H:%M:%S')}", flush=True)
        log_resource_usage("WARMUP_START")
        
        try:
            warmup_text = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚"
            if llamacpp_engine is None:
                raise RuntimeError("Llama.cpp engine not initialized")
            warmup_result = llamacpp_engine.repair(warmup_text)
            model_warmed = True
            warmup_time = time.time() - step_start
            print(f"[Semantic Repair ZH] âœ… Model warm-up completed (took {warmup_time:.2f}s)", flush=True)
            log_resource_usage("WARMUP_COMPLETE")
        except Exception as e:
            warmup_time = time.time() - step_start
            print(f"[Semantic Repair ZH] âš ï¸  Warm-up failed after {warmup_time:.2f}s (will warm-up on first request): {e}", flush=True)
            import traceback
            traceback.print_exc()
            model_warmed = False
            log_resource_usage("WARMUP_FAILED")
        
        total_startup_time = time.time() - startup_start_time
        print(f"[Semantic Repair ZH] âœ… Service is ready (total startup time: {total_startup_time:.2f}s)", flush=True)
        print(f"[Semantic Repair ZH] ğŸ’¡  Breakdown: Device setup: {time.time() - startup_start_time:.2f}s, Model load: {model_load_time:.2f}s, Warm-up: {warmup_time:.2f}s", flush=True)
        log_resource_usage("READY")
    except Exception as e:
        total_startup_time = time.time() - startup_start_time
        print(f"[Semantic Repair ZH] [CRITICAL ERROR] Failed to initialize after {total_startup_time:.2f}s: {e}", flush=True)
        import traceback
        traceback.print_exc()
        log_resource_usage("ERROR")
        raise
    
    yield  # åº”ç”¨è¿è¡ŒæœŸé—´
    
    # ==================== å…³é—­æ—¶æ‰§è¡Œï¼ˆä¼˜é›…å…³é—­ï¼‰ ====================
    try:
        print("[Semantic Repair ZH] ===== Shutting down Semantic Repair Service (Chinese) =====", flush=True)
        print(f"[Semantic Repair ZH] Main process PID: {os.getpid()}", flush=True)
        
        # æ¸…ç† llama.cpp å¼•æ“
        if llamacpp_engine is not None:
            print("[Semantic Repair ZH] Cleaning up llama.cpp engine...", flush=True)
            llamacpp_engine.shutdown()
            llamacpp_engine = None
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        print("[Semantic Repair ZH] âœ… Graceful shutdown completed", flush=True)
    except Exception as e:
        print(f"[Semantic Repair ZH] âŒ Error during shutdown: {e}", flush=True)
        import traceback
        traceback.print_exc()


# åˆ›å»º FastAPI åº”ç”¨ï¼ˆä½¿ç”¨lifespanæ›¿ä»£@app.on_eventï¼‰
app = FastAPI(
    title="Semantic Repair Service - Chinese",
    version="1.0.0",
    lifespan=lifespan
)


# ==================== è¾…åŠ©ï¼šæ˜¯å¦ç¡®æœ‰æ”¹å–„ ====================

def _output_actually_improved(text_in: str, text_out: str) -> bool:
    """ä»…å½“è¾“å‡ºç›¸å¯¹è¾“å…¥ç¡®æœ‰æ”¹å–„æ—¶è¿”å› Trueï¼ˆå¦‚ç¹â†’ç®€ã€åŒéŸ³å­—ä¿®æ­£ï¼‰ï¼Œé¿å…æœªä¿®å¤ä¹Ÿæ ‡ REPAIRã€‚"""
    if text_out == text_in:
        return False
    # å¸¸è§ç¹ä½“å­—ï¼ˆä¸ç®€ä½“å¯¹åº”ï¼‰ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦åšäº†ç¹â†’ç®€
    trad = set("æˆ‘å€‘æœƒä¾†èªªé€™å€‹å€‘æ™‚å‹•è­˜è®€èªéé•·æ–·ç¯€ç·´ç¿’é ‚ç¶“ç‡Ÿè§£ç’°çµ¦èªŒèˆ‡æ–¼ç‚º")
    n_in = sum(1 for c in text_in if c in trad)
    n_out = sum(1 for c in text_out if c in trad)
    if n_in > 0 and n_out >= n_in:
        return False
    return True


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class RepairRequest(BaseModel):
    """ä¿®å¤è¯·æ±‚"""
    job_id: str
    session_id: str
    utterance_index: int = 0
    lang: str = Field(default="zh", description="è¯­è¨€ä»£ç ")
    text_in: str = Field(..., description="è¾“å…¥æ–‡æœ¬")
    quality_score: Optional[float] = Field(default=None, description="è´¨é‡åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰")
    micro_context: Optional[str] = Field(default=None, description="å¾®ä¸Šä¸‹æ–‡ï¼ˆä¸Šä¸€å¥å°¾éƒ¨ï¼‰")
    meta: Optional[Dict] = Field(default=None, description="å…ƒæ•°æ®")


class RepairResponse(BaseModel):
    """ä¿®å¤å“åº”"""
    decision: str = Field(..., description="å†³ç­–ï¼šPASSã€REPAIR æˆ– REJECT")
    text_out: str = Field(..., description="è¾“å‡ºæ–‡æœ¬")
    confidence: float = Field(..., description="ç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰")
    diff: List[Dict] = Field(default_factory=list, description="å·®å¼‚åˆ—è¡¨")
    reason_codes: List[str] = Field(default_factory=list, description="åŸå› ä»£ç åˆ—è¡¨")
    repair_time_ms: Optional[int] = Field(default=None, description="ä¿®å¤è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰")


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field(..., description="çŠ¶æ€ï¼šhealthyã€loading æˆ– error")
    model_loaded: bool = Field(..., description="æ¨¡å‹æ˜¯å¦å·²åŠ è½½")
    model_version: Optional[str] = Field(default=None, description="æ¨¡å‹ç‰ˆæœ¬")
    warmed: bool = Field(default=False, description="æ¨¡å‹æ˜¯å¦å·²warm")


# ==================== API ç«¯ç‚¹ ====================
# æ³¨æ„ï¼šå¯åŠ¨å’Œå…³é—­é€»è¾‘å·²ç§»è‡³ lifespan ä¸Šä¸‹æ–‡ç®¡ç†å™¨

@app.post("/repair", response_model=RepairResponse)
async def repair_text(request: RepairRequest):
    """
    ä¿®å¤ASRæ–‡æœ¬
    
    å¯¹ASRè¾“å‡ºçš„ä¸­æ–‡æ–‡æœ¬è¿›è¡Œè¯­ä¹‰ä¿®å¤ï¼Œä¸»è¦è§£å†³åŒéŸ³å­—é”™è¯¯ã€ä¸“æœ‰åè¯è¯¯è¯†åˆ«ç­‰é—®é¢˜ã€‚
    """
    global llamacpp_engine, model_warmed
    
    # æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨
    if llamacpp_engine is None:
        raise HTTPException(status_code=503, detail="Llama.cpp engine not initialized")
    
    # åªå¤„ç†ä¸­æ–‡
    if request.lang != "zh":
        return RepairResponse(
            decision="PASS",
            text_out=request.text_in,
            confidence=1.0,
            reason_codes=["NOT_CHINESE"],
        )
    
    # æ¨¡å‹é¢„çƒ­ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶ï¼‰
    if not model_warmed:
        try:
            # llama.cpp ä¸éœ€è¦å•ç‹¬çš„ warm_upï¼Œç›´æ¥ä½¿ç”¨å³å¯
            model_warmed = True
        except Exception as e:
            print(f"[Semantic Repair ZH] Warm-up failed: {e}", flush=True)
    
    start_time = time.time()
    
    # è®°å½•è¾“å…¥ï¼ˆä»»åŠ¡é“¾æ—¥å¿—ï¼‰- åŒæ—¶ä½¿ç”¨printç¡®ä¿è¾“å‡ºå¯è§
    input_log = (
        f"SEMANTIC_REPAIR_ZH INPUT: Received repair request | "
        f"job_id={request.job_id} | "
        f"session_id={request.session_id} | "
        f"utterance_index={request.utterance_index} | "
        f"lang={request.lang} | "
        f"text_in={request.text_in!r} | "
        f"text_in_length={len(request.text_in)} | "
        f"quality_score={request.quality_score} | "
        f"micro_context={repr(request.micro_context) if request.micro_context else None}"
    )
    logger.info(input_log)
    print(f"[Semantic Repair ZH] {input_log}", flush=True)
    
    try:
        # æ‰§è¡Œä¿®å¤
        result = llamacpp_engine.repair(
            text_in=request.text_in,
            micro_context=request.micro_context,
            quality_score=request.quality_score
        )
        
        elapsed_ms = int((time.time() - start_time) * 1000)

        # ä»…å½“è¾“å‡ºä¸è¾“å…¥ä¸åŒä¸”ç¡®æœ‰æ”¹å–„æ—¶æ‰æ ‡ REPAIRï¼Œé¿å…æœªä¿®å¤å†…å®¹è¢«æ ‡ä¸ºå·²ä¿®å¤
        text_out = result['text_out']
        decision = "PASS"
        if text_out != request.text_in:
            if _output_actually_improved(request.text_in, text_out):
                decision = "REPAIR"
            else:
                text_out = request.text_in
                logger.info(
                    "[Semantic Repair ZH] Output unchanged or not improved (e.g. still traditional), using PASS and original text"
                )
        reason_codes = []

        if request.quality_score is not None and request.quality_score < 0.85:
            reason_codes.append("LOW_QUALITY_SCORE")
        if decision == "REPAIR":
            reason_codes.append("REPAIR_APPLIED")
        
        # è®°å½•è¾“å‡ºï¼ˆä»»åŠ¡é“¾æ—¥å¿—ï¼‰- åŒæ—¶ä½¿ç”¨printç¡®ä¿è¾“å‡ºå¯è§
        output_log = (
            f"SEMANTIC_REPAIR_ZH OUTPUT: Repair completed | "
            f"job_id={request.job_id} | "
            f"session_id={request.session_id} | "
            f"utterance_index={request.utterance_index} | "
            f"decision={decision} | "
            f"text_out={text_out!r} | "
            f"text_out_length={len(text_out)} | "
            f"confidence={result['confidence']:.2f} | "
            f"reason_codes={reason_codes} | "
            f"repair_time_ms={elapsed_ms} | "
            f"changed={text_out != request.text_in}"
        )
        logger.info(output_log)
        print(f"[Semantic Repair ZH] {output_log}", flush=True)

        return RepairResponse(
            decision=decision,
            text_out=text_out,
            confidence=result['confidence'],
            diff=result['diff'],
            reason_codes=reason_codes,
            repair_time_ms=elapsed_ms,
        )
    except Exception as e:
        print(f"[Semantic Repair ZH] Error during repair: {e}", flush=True)
        import traceback
        traceback.print_exc()
        
        # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›åŸæ–‡
        return RepairResponse(
            decision="PASS",
            text_out=request.text_in,
            confidence=0.5,
            reason_codes=["ERROR"],
            repair_time_ms=int((time.time() - start_time) * 1000),
        )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    è¿”å›æœåŠ¡å¥åº·çŠ¶æ€å’Œæ¨¡å‹warmçŠ¶æ€
    åªæœ‰åœ¨warm-upå®Œæˆåæ‰è¿”å›"healthy"çŠ¶æ€
    """
    global llamacpp_engine, loaded_model_path, model_warmed
    
    if llamacpp_engine is None:
        return HealthResponse(
            status="loading",  # å¼•æ“æ­£åœ¨åŠ è½½ä¸­
            model_loaded=False,
            warmed=False,
        )
    
    # å¦‚æœå¼•æ“å·²åŠ è½½ä½†æœªå®Œæˆwarm-upï¼Œè¿”å›"loading"çŠ¶æ€
    if not model_warmed:
        return HealthResponse(
            status="loading",  # å¼•æ“å·²åŠ è½½ï¼Œä½†æ­£åœ¨warm-upä¸­
            model_loaded=True,
            warmed=False,
        )
    
    model_version = "qwen2.5-3b-instruct-zh-gguf"
    if loaded_model_path:
        # ä»è·¯å¾„æå–æ¨¡å‹ç‰ˆæœ¬
        model_name = os.path.basename(loaded_model_path)
        if model_name:
            model_version = model_name
    
    # åªæœ‰åœ¨warm-upå®Œæˆåæ‰è¿”å›"healthy"
    return HealthResponse(
        status="healthy",
        model_loaded=True,
        model_version=model_version,
        warmed=model_warmed,
    )


class DiagnosticsResponse(BaseModel):
    """è¯Šæ–­ä¿¡æ¯å“åº”"""
    device: str = Field(..., description="è®¾å¤‡ç±»å‹")
    device_name: Optional[str] = Field(default=None, description="è®¾å¤‡åç§°")
    gpu_memory_allocated_gb: Optional[float] = Field(default=None, description="GPUå·²åˆ†é…å†…å­˜(GB)")
    gpu_memory_reserved_gb: Optional[float] = Field(default=None, description="GPUå·²ä¿ç•™å†…å­˜(GB)")
    gpu_memory_total_gb: Optional[float] = Field(default=None, description="GPUæ€»å†…å­˜(GB)")
    engine: Optional[str] = Field(default=None, description="ä½¿ç”¨çš„å¼•æ“")
    model_path: Optional[str] = Field(default=None, description="æ¨¡å‹è·¯å¾„")
    quantization_enabled: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨é‡åŒ–")
    process_memory_mb: Optional[float] = Field(default=None, description="è¿›ç¨‹å†…å­˜ä½¿ç”¨(MB)")
    cuda_available: bool = Field(default=False, description="CUDAæ˜¯å¦å¯ç”¨")


@app.get("/diagnostics", response_model=DiagnosticsResponse)
async def diagnostics():
    """
    è¯Šæ–­ç«¯ç‚¹
    
    è¿”å›è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯ï¼ŒåŒ…æ‹¬è®¾å¤‡ã€å†…å­˜ä½¿ç”¨ç­‰
    """
    global llamacpp_engine, DEVICE, loaded_model_path
    
    diagnostics_data = {
        "device": str(DEVICE) if DEVICE else "unknown",
        "cuda_available": torch.cuda.is_available() if DEVICE and DEVICE.type == "cuda" else False,
    }
    
    # è·å–GPUä¿¡æ¯
    if DEVICE and DEVICE.type == "cuda" and torch.cuda.is_available():
        try:
            diagnostics_data["device_name"] = torch.cuda.get_device_name(0)
            diagnostics_data["gpu_memory_allocated_gb"] = torch.cuda.memory_allocated() / 1024**3
            diagnostics_data["gpu_memory_reserved_gb"] = torch.cuda.memory_reserved() / 1024**3
            diagnostics_data["gpu_memory_total_gb"] = torch.cuda.get_device_properties(0).total_memory / 1024**3
        except Exception as e:
            print(f"[Semantic Repair ZH] Error getting GPU info: {e}", flush=True)
    
    # è·å– llama.cpp å¼•æ“ä¿¡æ¯
    if llamacpp_engine is not None:
        try:
            health = llamacpp_engine.health()
            diagnostics_data["engine"] = health.get("engine", "llamacpp")
            diagnostics_data["model_path"] = health.get("model_path", loaded_model_path)
            diagnostics_data["quantization_enabled"] = True  # GGUF æ¨¡å‹æ€»æ˜¯é‡åŒ–çš„
        except Exception as e:
            print(f"[Semantic Repair ZH] Error getting engine info: {e}", flush=True)
    
    # è·å–è¿›ç¨‹å†…å­˜ä½¿ç”¨
    try:
        import psutil
        process = psutil.Process()
        diagnostics_data["process_memory_mb"] = process.memory_info().rss / 1024 / 1024
    except Exception as e:
        print(f"[Semantic Repair ZH] Error getting process memory: {e}", flush=True)
    
    return DiagnosticsResponse(**diagnostics_data)


# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    import uvicorn
    
    # ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–ç«¯å£
    port = int(os.environ.get("PORT", 5013))
    host = os.environ.get("HOST", "127.0.0.1")
    
    print(f"[Semantic Repair ZH] Starting server on {host}:{port}", flush=True)
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        workers=1,  # å•è¿›ç¨‹ï¼Œé¿å…å¤šè¿›ç¨‹å¯¼è‡´çš„é«˜CPUå ç”¨
        loop="asyncio",  # ä½¿ç”¨asyncioäº‹ä»¶å¾ªç¯
    )
