# ASR 同音字修复：除文本语义模型外的其他方式

## 背景

当前使用 Qwen2.5-Instruct 做「纯文本语义修复」：输入 ASR 文本，模型自由生成修正。问题包括：

- **同音字纠偏不足**：如「余英」应改为「语音」，模型却改为「英文」，严重偏离原意。
- **过度依赖通用语义**：通用 instruct 模型未针对 ASR 同音字分布优化，易误改或漏改。

除这类「端到端文本语义修复模型」外，还有多种更可控的修复思路。下文仅保留**在既定约束下可行**的方案。

---

## 约束说明（不做、不支持、不采用）

- **不做词汇表/规则表**：不维护「原文片段 → 修正」之类的手动映射表，避免系统难以维护。
- **faster-whisper 不支持 N-best**：无法利用 N-best 或词图做重排序。
- **不做二次识别**：不再跑一遍 ASR，否则耗时会成倍增加，得不偿失。

因此：规则词典、N-best 重排、二次 ASR 等方案**不纳入**下文推荐。

---

## 一、拼音自动混淆集 + 语言模型（无词表维护）

思路：同音字错误在**发音上一致**，纠错时只允许「同音/近音」替换，从机制上避免「余英→英文」这类跨音误改。

- **混淆集从哪来**：**不是**手写词表。用标准**汉字→拼音**映射（如 pypinyin、静态拼音表）自动得到「同音字组」：每个字的候选 = 同拼音的其他字。例如「余」的候选 = 同音字集合 {语、鱼、于、余…}，「英」的候选 = {音、英、应、鹰…}。数据只需一份通用拼音表，无需按业务维护「余英→语音」之类映射。
- **做法**：对 ASR 句子逐字（或按词）得到同音候选，用**中文 n-gram 或神经语言模型**对整句候选组合打分，选分数最高的句子；多字同时错时可用束搜索或贪心限制搜索空间。
- **优点**：只做同音替换，不会出现「余英→英文」；无业务词表维护；单遍、不依赖 ASR 的 N-best 或二次识别。
- **缺点**：需要一份 LM 与拼音数据；多字错时搜索空间需用策略控制。

---

## 二、专用中文拼写/纠错模型（CSC）

思路：用**面向中文拼写纠错（CSC）**训练的模型做「只改错字、少改原句」，替代或前置通用生成模型。

- **代表**：MacBERT4CSC、ChineseBERT-for-CSC、Speller-BERT 等；工具箱如 [csc_tools](https://github.com/iioSnail/csc_tools)。
- **特点**：训练目标就是最小编辑、纠错，不做自由生成；单遍推理，无需 N-best、无需二次 ASR；**无词表维护**，模型即策略。
- **落地**：可单独部署一个 CSC 服务，对 1-best ASR 文本做一次纠错；可替代当前 Qwen 语义修复，或放在 Qwen 前做同音字/拼写层。

---

## 三、拼音引导的纠错模型（研究/开源）

思路：模型输入包含**汉字 + 拼音（或发音特征）**，约束「只改字不改音」。

- 拼音可从**当前 ASR 文本**用 pypinyin 等得到（不需要 ASR 引擎输出拼音），再喂给拼音引导的纠错模型；不依赖 N-best 或二次识别。
- **缺点**：需接入相应模型与推理管线；若暂无现成可部署实现，可作为中长期选项。

---

## 建议优先级（在「无词表、无 N-best、无二次 ASR」前提下）

| 优先级 | 方式 | 说明 |
|--------|------|------|
| 1 | **拼音自动混淆集 + LM** | 混淆集由拼音表自动生成，无业务词表；同音约束强，单遍、不增加 ASR 耗时。 |
| 2 | **CSC 专用模型** | 无词表、单遍、不依赖 N-best；用现成 CSC 模型替代或前置当前 Qwen 语义修复。 |
| 3 | **拼音引导纠错模型** | 若有现成可部署实现，可作为中长期选项，效果通常优于纯语义生成。 |

当前「余英→语音」这类问题，用**拼音自动混淆集 + LM** 即可在无词表维护的前提下避免误改成「英文」；若希望尽量少自研打分与搜索逻辑，可优先评估 **CSC 模型** 替代或前置现有语义修复。
