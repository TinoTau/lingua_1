//! æ¨ç†æœåŠ¡æ ¸å¿ƒç±»å‹å’Œå®ç°

use anyhow::Result;
use serde::{Deserialize, Serialize};  // Deserialize åœ¨ InferenceRequest/InferenceResult ä¸­ä½¿ç”¨ï¼ˆç”¨äº JSON ååºåˆ—åŒ–ï¼‰
use std::path::PathBuf;
use tracing::{info, warn, debug};

use crate::modules::{FeatureSet, ModuleManager, InferenceModule};
use crate::pipeline::PipelineContext;
use crate::asr;
use crate::nmt;
use crate::tts;
use crate::vad;
use crate::speaker;
use crate::speech_rate;
use crate::language_detector;
use std::sync::Arc;

/// éƒ¨åˆ†ç»“æœå›è°ƒå‡½æ•°ç±»å‹
pub type PartialResultCallback = Arc<dyn Fn(asr::ASRPartialResult) + Send + Sync>;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    pub job_id: String,
    pub src_lang: String,  // æ”¯æŒ "auto" | "zh" | "en" | "ja" | "ko"
    pub tgt_lang: String,
    pub audio_data: Vec<u8>, // PCM audio data
    pub features: Option<FeatureSet>, // å¯é€‰åŠŸèƒ½è¯·æ±‚
    /// ç¿»è¯‘æ¨¡å¼ï¼š"one_way" | "two_way_auto"
    #[serde(skip_serializing_if = "Option::is_none")]
    pub mode: Option<String>,
    /// åŒå‘æ¨¡å¼çš„è¯­è¨€ Aï¼ˆå½“ mode == "two_way_auto" æ—¶ä½¿ç”¨ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub lang_a: Option<String>,
    /// åŒå‘æ¨¡å¼çš„è¯­è¨€ Bï¼ˆå½“ mode == "two_way_auto" æ—¶ä½¿ç”¨ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub lang_b: Option<String>,
    /// è‡ªåŠ¨è¯†åˆ«æ—¶é™åˆ¶çš„è¯­è¨€èŒƒå›´ï¼ˆå¯é€‰ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub auto_langs: Option<Vec<String>>,
    /// æ˜¯å¦å¯ç”¨æµå¼ ASRï¼ˆéƒ¨åˆ†ç»“æœè¾“å‡ºï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub enable_streaming_asr: Option<bool>,
    /// éƒ¨åˆ†ç»“æœæ›´æ–°é—´éš”ï¼ˆæ¯«ç§’ï¼‰ï¼Œä»…åœ¨ enable_streaming_asr ä¸º true æ—¶æœ‰æ•ˆ
    #[serde(skip_serializing_if = "Option::is_none")]
    pub partial_update_interval_ms: Option<u64>,
    /// è¿½è¸ª IDï¼ˆç”¨äºå…¨é“¾è·¯æ—¥å¿—è¿½è¸ªï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub trace_id: Option<String>,
    /// ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œç”¨äº NMT ç¿»è¯‘è´¨é‡æå‡ï¼‰
    #[serde(skip_serializing_if = "Option::is_none")]
    pub context_text: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResult {
    pub transcript: String,
    pub translation: String,
    pub audio: Vec<u8>, // TTS audio data
    // å¯é€‰ç»“æœ
    pub speaker_id: Option<String>,
    pub speech_rate: Option<f32>,
    pub emotion: Option<String>,
}

pub struct InferenceService {
    // æ ¸å¿ƒæ¨¡å—ï¼ˆå¿…éœ€ï¼‰
    asr_engine: asr::ASREngine,
    nmt_engine: nmt::NMTEngine,
    tts_engine: tts::TTSEngine,
    vad_engine: vad::VADEngine,  // VAD ç”¨äºèŠ‚ç‚¹ç«¯ Level 2 æ–­å¥ã€è¯­éŸ³æ®µæå–å’Œä¸Šä¸‹æ–‡ç¼“å†²åŒºä¼˜åŒ–
    
    // è¯­è¨€æ£€æµ‹å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºè‡ªåŠ¨è¯­ç§è¯†åˆ«ï¼‰
    language_detector: Option<language_detector::LanguageDetector>,
    
    // å¯é€‰æ¨¡å—ï¼ˆä½¿ç”¨ Arc<RwLock<>> ä»¥æ”¯æŒå¹¶å‘è®¿é—®å’ŒåŠ¨æ€ä¿®æ”¹ï¼‰
    speaker_identifier: Option<std::sync::Arc<tokio::sync::RwLock<speaker::SpeakerIdentifier>>>,
    voice_cloner: Option<std::sync::Arc<tokio::sync::RwLock<speaker::VoiceCloner>>>,
    speech_rate_detector: Option<std::sync::Arc<tokio::sync::RwLock<speech_rate::SpeechRateDetector>>>,
    speech_rate_controller: Option<std::sync::Arc<tokio::sync::RwLock<speech_rate::SpeechRateController>>>,
    
    // æ¨¡å—ç®¡ç†å™¨
    module_manager: ModuleManager,
    
    // ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼šä¿å­˜å‰ä¸€ä¸ªutteranceçš„å°¾éƒ¨éŸ³é¢‘ï¼ˆç”¨äºæé«˜ASRå‡†ç¡®æ€§ï¼‰
    // é‡‡æ ·ç‡ï¼š16kHzï¼Œæ ¼å¼ï¼šf32ï¼ŒèŒƒå›´ï¼š[-1.0, 1.0]
    // æœ€å¤§é•¿åº¦ï¼š2ç§’ï¼ˆ32000ä¸ªæ ·æœ¬ @ 16kHzï¼‰
    context_buffer: Arc<tokio::sync::Mutex<Vec<f32>>>,
}

impl InferenceService {
    pub fn new(models_dir: PathBuf) -> Result<Self> {
        // ASR æ¨¡å‹åœ¨ whisper-base å­ç›®å½•ä¸­
        let asr_engine = asr::ASREngine::new(models_dir.join("asr").join("whisper-base"))?;
        // ä½¿ç”¨ HTTP å®¢æˆ·ç«¯æ–¹å¼åˆå§‹åŒ– NMTï¼ˆæ¨èï¼‰
        let nmt_engine = nmt::NMTEngine::new_with_http_client(None)?;
        let tts_engine = tts::TTSEngine::new(None)?;
        // VAD æ¨¡å‹åœ¨ silero å­ç›®å½•ä¸­
        let vad_engine = vad::VADEngine::new(models_dir.join("vad").join("silero"))?;

        // åˆå§‹åŒ–è¯­è¨€æ£€æµ‹å™¨ï¼ˆå¤ç”¨ ASR å¼•æ“çš„ Whisper ä¸Šä¸‹æ–‡ï¼‰
        let whisper_ctx = asr_engine.get_whisper_ctx();
        let language_detector = Some(language_detector::LanguageDetector::new(
            whisper_ctx,
            None,  // ä½¿ç”¨é»˜è®¤é…ç½®
        ));

        let module_manager = ModuleManager::new();

        // åˆå§‹åŒ–å¯é€‰æ¨¡å—ï¼ˆå³ä½¿æœªå¯ç”¨ï¼Œä¹Ÿåˆ›å»ºå®ä¾‹ä»¥ä¾¿åç»­å¯ç”¨ï¼‰
        let speaker_identifier = Some(std::sync::Arc::new(tokio::sync::RwLock::new(speaker::SpeakerIdentifier::new())));
        let voice_cloner = Some(std::sync::Arc::new(tokio::sync::RwLock::new(speaker::VoiceCloner::new())));

        Ok(Self {
            asr_engine,
            nmt_engine,
            tts_engine,
            vad_engine,
            language_detector,
            speaker_identifier,
            voice_cloner,
            speech_rate_detector: None,
            speech_rate_controller: None,
            module_manager,
            context_buffer: Arc::new(tokio::sync::Mutex::new(Vec::new())),
        })
    }

    /// å¯ç”¨æ¨¡å—ï¼ˆå®Œæ•´æµç¨‹ï¼‰
    /// 
    /// æŒ‰ç…§ v2 æŠ€æœ¯è¯´æ˜ä¹¦çš„è¦æ±‚ï¼š
    /// 1. ä½¿ç”¨ ModuleManager è¿›è¡Œä¾èµ–æ£€æŸ¥ã€å†²çªæ£€æŸ¥ã€æ¨¡å‹æ£€æŸ¥
    /// 2. å¦‚æœæ£€æŸ¥é€šè¿‡ï¼ŒåŠ è½½æ¨¡å—æ¨¡å‹
    /// 3. æ ‡è®°æ¨¡å—ä¸ºå·²å¯ç”¨
    pub async fn enable_module(&self, module_name: &str) -> Result<()> {
        // æ­¥éª¤ 1: ä½¿ç”¨ ModuleManager è¿›è¡Œæ‰€æœ‰æ£€æŸ¥ï¼ˆä¾èµ–ã€å†²çªã€æ¨¡å‹ï¼‰
        self.module_manager.enable_module(module_name).await?;
        
        // æ­¥éª¤ 2: å¦‚æœæ£€æŸ¥é€šè¿‡ï¼ŒåŠ è½½æ¨¡å—æ¨¡å‹
        match module_name {
            "speaker_identification" => {
                if let Some(ref m) = self.speaker_identifier {
                    let mut module = m.write().await;
                    InferenceModule::enable(&mut *module).await?;
                } else {
                    // æ¨¡å—æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆ›å»ºï¼ˆcold-loadï¼‰
                    // TODO: å®ç°æ¨¡å—çš„å»¶è¿Ÿåˆå§‹åŒ–é€»è¾‘
                    return Err(anyhow::anyhow!("Module {} not initialized. Please initialize it first.", module_name));
                }
            }
            "voice_cloning" => {
                if let Some(ref m) = self.voice_cloner {
                    let mut module = m.write().await;
                    InferenceModule::enable(&mut *module).await?;
                } else {
                    return Err(anyhow::anyhow!("Module {} not initialized. Please initialize it first.", module_name));
                }
            }
            "speech_rate_detection" => {
                if let Some(ref m) = self.speech_rate_detector {
                    let mut module = m.write().await;
                    InferenceModule::enable(&mut *module).await?;
                } else {
                    return Err(anyhow::anyhow!("Module {} not initialized. Please initialize it first.", module_name));
                }
            }
            "speech_rate_control" => {
                if let Some(ref m) = self.speech_rate_controller {
                    let mut module = m.write().await;
                    InferenceModule::enable(&mut *module).await?;
                } else {
                    return Err(anyhow::anyhow!("Module {} not initialized. Please initialize it first.", module_name));
                }
            }
            "emotion_detection" => {
                // TODO: å®ç°æƒ…æ„Ÿæ£€æµ‹æ¨¡å—
                return Err(anyhow::anyhow!("Module {} not yet implemented.", module_name));
            }
            "persona_adaptation" => {
                // TODO: å®ç°ä¸ªæ€§åŒ–é€‚é…æ¨¡å—
                return Err(anyhow::anyhow!("Module {} not yet implemented.", module_name));
            }
            _ => return Err(anyhow::anyhow!("Unknown module: {}", module_name)),
        }
        
        // æ­¥éª¤ 3: æ›´æ–°æ¨¡å—çŠ¶æ€ä¸º model_loaded
        // æ³¨æ„ï¼šModuleManager çš„ states æ˜¯ç§æœ‰çš„ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼æ›´æ–°
        // å½“å‰å®ç°ä¸­ï¼ŒModuleManager::enable_module å·²ç»æ›´æ–°äº†çŠ¶æ€
        // æˆ‘ä»¬åªéœ€è¦ç¡®ä¿æ¨¡å—çš„ model_loaded æ ‡å¿—æ­£ç¡®è®¾ç½®
        
        Ok(())
    }

    pub async fn disable_module(&self, module_name: &str) -> Result<()> {
        match module_name {
            "speaker_identification" => {
                if let Some(ref m) = self.speaker_identifier {
                    let mut module = m.write().await;
                    InferenceModule::disable(&mut *module).await?;
                }
            }
            "voice_cloning" => {
                if let Some(ref m) = self.voice_cloner {
                    let mut module = m.write().await;
                    InferenceModule::disable(&mut *module).await?;
                }
            }
            "speech_rate_detection" => {
                if let Some(ref m) = self.speech_rate_detector {
                    let mut module = m.write().await;
                    InferenceModule::disable(&mut *module).await?;
                }
            }
            "speech_rate_control" => {
                if let Some(ref m) = self.speech_rate_controller {
                    let mut module = m.write().await;
                    InferenceModule::disable(&mut *module).await?;
                }
            }
            _ => return Err(anyhow::anyhow!("Unknown module: {}", module_name)),
        }
        self.module_manager.disable_module(module_name).await?;
        Ok(())
    }

    /// æ¸…ç©ºä¸Šä¸‹æ–‡ç¼“å†²åŒº
    /// 
    /// ç”¨äºä¼šè¯ç»“æŸæˆ–éœ€è¦é‡ç½®ä¸Šä¸‹æ–‡æ—¶è°ƒç”¨
    pub async fn clear_context_buffer(&self) {
        let mut context = self.context_buffer.lock().await;
        let previous_size = context.len();
        context.clear();
        // åŒæ—¶é‡ç½®VADçŠ¶æ€
        if let Err(e) = self.vad_engine.reset_state() {
            tracing::warn!("é‡ç½®VADçŠ¶æ€å¤±è´¥: {}", e);
        }
        info!(
            previous_context_samples = previous_size,
            previous_context_duration_sec = (previous_size as f32 / 16000.0),
            "ğŸ—‘ï¸ ä¸Šä¸‹æ–‡ç¼“å†²åŒºå’ŒVADçŠ¶æ€å·²æ¸…ç©º"
        );
    }

    /// è·å–ä¸Šä¸‹æ–‡ç¼“å†²åŒºå½“å‰å¤§å°ï¼ˆæ ·æœ¬æ•°ï¼‰
    pub async fn get_context_buffer_size(&self) -> usize {
        let context = self.context_buffer.lock().await;
        context.len()
    }

    /// å¤„ç†æ¨ç†è¯·æ±‚ï¼ˆæ”¯æŒéƒ¨åˆ†ç»“æœå›è°ƒï¼‰
    /// 
    /// # Arguments
    /// * `request` - æ¨ç†è¯·æ±‚
    /// * `partial_callback` - éƒ¨åˆ†ç»“æœå›è°ƒï¼ˆå¯é€‰ï¼‰
    pub async fn process(&self, request: InferenceRequest, partial_callback: Option<PartialResultCallback>) -> Result<InferenceResult> {
        // ä½¿ç”¨ trace_id è¿›è¡Œæ—¥å¿—è®°å½•ï¼ˆå¦‚æœæä¾›ï¼‰
        let trace_id = request.trace_id.as_deref().unwrap_or("unknown");
        use tracing::{info, warn, debug};
        
        debug!(trace_id = %trace_id, job_id = %request.job_id, "å¼€å§‹å¤„ç†æ¨ç†è¯·æ±‚");
        
        // æ ¹æ®è¯·æ±‚ä¸­çš„ features è‡ªåŠ¨å¯ç”¨æ‰€éœ€æ¨¡å—ï¼ˆè¿è¡Œæ—¶åŠ¨æ€å¯ç”¨ï¼‰
        if let Some(ref features) = request.features {
            // æ ¹æ®ä»»åŠ¡éœ€æ±‚è‡ªåŠ¨å¯ç”¨æ¨¡å—
            if features.speaker_identification {
                let _ = self.enable_module("speaker_identification").await;
            }
            if features.voice_cloning {
                let _ = self.enable_module("voice_cloning").await;
            }
            if features.speech_rate_detection {
                let _ = self.enable_module("speech_rate_detection").await;
            }
            if features.speech_rate_control {
                let _ = self.enable_module("speech_rate_control").await;
            }
            // TODO: å½“å®ç°æƒ…æ„Ÿæ£€æµ‹å’Œä¸ªæ€§åŒ–é€‚é…æ¨¡å—æ—¶ï¼Œæ·»åŠ ç›¸åº”çš„å¯ç”¨é€»è¾‘
            // if features.emotion_detection {
            //     let _ = self.enable_module("emotion_detection").await;
            // }
            // if features.persona_adaptation {
            //     let _ = self.enable_module("persona_adaptation").await;
            // }
        }
        
        // ä½¿ç”¨ PipelineContext ç»Ÿä¸€ç®¡ç†æ•°æ®æµ
        let mut ctx = PipelineContext::from_audio(request.audio_data.clone());
        let features = request.features.as_ref();
        
        // å°† PCM 16-bit è½¬æ¢ä¸º f32ï¼ˆç”¨äºè¯­è¨€æ£€æµ‹å’Œ ASRï¼‰
        let audio_f32: Vec<f32> = request.audio_data
            .chunks_exact(2)
            .map(|chunk| {
                let sample = i16::from_le_bytes([chunk[0], chunk[1]]) as f32;
                sample / 32768.0
            })
            .collect();
        
        let mut src_lang = request.src_lang.clone();
        let mut tgt_lang = request.tgt_lang.clone();
        
        // 1. è¯­è¨€æ£€æµ‹ï¼ˆå¦‚æœ src_lang == "auto"ï¼‰
        if src_lang == "auto" {
            debug!(trace_id = %trace_id, "å¼€å§‹è¯­è¨€æ£€æµ‹");
            if let Some(ref detector) = self.language_detector {
                match detector.detect(&audio_f32, 16000).await {
                    Ok(detection) => {
                        info!(trace_id = %trace_id, lang = %detection.lang, confidence = %detection.confidence, "è¯­è¨€æ£€æµ‹å®Œæˆ");
                        src_lang = detection.lang.clone();
                        
                        // åŒå‘æ¨¡å¼ï¼šæ ¹æ®æ£€æµ‹ç»“æœé€‰æ‹©ç¿»è¯‘æ–¹å‘
                        if let Some(ref mode) = request.mode {
                            if mode == "two_way_auto" {
                                if let (Some(ref lang_a), Some(ref lang_b)) = (&request.lang_a, &request.lang_b) {
                                    if src_lang == *lang_a {
                                        tgt_lang = lang_b.clone();
                                        info!("Two-way mode: {} -> {}", src_lang, tgt_lang);
                                    } else if src_lang == *lang_b {
                                        tgt_lang = lang_a.clone();
                                        info!("Two-way mode: {} -> {}", src_lang, tgt_lang);
                                    } else {
                                        // éä¸»è¦è¯­è¨€ï¼Œä½¿ç”¨é»˜è®¤ç›®æ ‡è¯­è¨€æˆ– lang_a
                                        use tracing::warn;
                                        tgt_lang = lang_a.clone();
                                        warn!("Detected language {} not in two-way pair, using default target: {}", src_lang, tgt_lang);
                                    }
                                }
                            }
                        }
                    }
                    Err(e) => {
                        warn!(trace_id = %trace_id, error = %e, default_lang = %src_lang, "è¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€");
                        // ä½¿ç”¨é»˜è®¤è¯­è¨€ï¼ˆå¦‚æœé…ç½®äº† auto_langsï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªï¼‰
                        if let Some(ref auto_langs) = request.auto_langs {
                            if !auto_langs.is_empty() {
                                src_lang = auto_langs[0].clone();
                            }
                        }
                    }
                }
            } else {
                warn!(trace_id = %trace_id, "è¯­è¨€æ£€æµ‹è¯·æ±‚ä½†æ£€æµ‹å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€");
                // å¦‚æœæ²¡æœ‰è¯­è¨€æ£€æµ‹å™¨ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€
                if let Some(ref auto_langs) = request.auto_langs {
                    if !auto_langs.is_empty() {
                        src_lang = auto_langs[0].clone();
                    }
                }
            }
        }
        
        // 2. ASR: è¯­éŸ³è¯†åˆ«ï¼ˆå¿…éœ€ï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€ï¼‰
        debug!(trace_id = %trace_id, src_lang = %src_lang, "å¼€å§‹ ASR è¯­éŸ³è¯†åˆ«");
        
        // 2.0 ä¸Šä¸‹æ–‡ç¼“å†²åŒºå¤„ç†ï¼šå‰ç½®å‰ä¸€ä¸ªutteranceçš„å°¾éƒ¨éŸ³é¢‘
        // è¿™å¯ä»¥æé«˜Whisperå¯¹å¥å­å¼€å¤´çš„è¯†åˆ«å‡†ç¡®æ€§
        let audio_f32_with_context = {
            let context = self.context_buffer.lock().await;
            if !context.is_empty() {
                let mut audio_with_context = context.clone();
                audio_with_context.extend_from_slice(&audio_f32);
                info!(
                    trace_id = %trace_id,
                    context_samples = context.len(),
                    context_duration_sec = (context.len() as f32 / 16000.0),
                    original_samples = audio_f32.len(),
                    original_duration_sec = (audio_f32.len() as f32 / 16000.0),
                    total_samples = audio_with_context.len(),
                    total_duration_sec = (audio_with_context.len() as f32 / 16000.0),
                    "âœ… å‰ç½®ä¸Šä¸‹æ–‡éŸ³é¢‘åˆ°å½“å‰utteranceï¼ˆä¸Šä¸‹æ–‡ç¼“å†²åŒºä¸ä¸ºç©ºï¼‰"
                );
                audio_with_context
            } else {
                info!(
                    trace_id = %trace_id,
                    original_samples = audio_f32.len(),
                    original_duration_sec = (audio_f32.len() as f32 / 16000.0),
                    "â„¹ï¸ ä¸Šä¸‹æ–‡ç¼“å†²åŒºä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘ï¼ˆç¬¬ä¸€ä¸ªutteranceæˆ–ä¸Šä¸‹æ–‡å·²æ¸…ç©ºï¼‰"
                );
                audio_f32.clone()
            }
        };
        
        // 2.0.1 ä½¿ç”¨VADæ£€æµ‹æœ‰æ•ˆè¯­éŸ³æ®µï¼ˆLevel 2æ–­å¥ï¼‰
        // æå–æœ‰æ•ˆè¯­éŸ³æ®µï¼Œå»é™¤é™éŸ³éƒ¨åˆ†ï¼Œæé«˜ASRå‡†ç¡®æ€§
        let audio_f32_processed = {
            match self.vad_engine.detect_speech(&audio_f32_with_context) {
                Ok(segments) => {
                    if segments.is_empty() {
                        warn!(
                            trace_id = %trace_id,
                            "VADæœªæ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œä½¿ç”¨å®Œæ•´éŸ³é¢‘è¿›è¡ŒASR"
                        );
                        audio_f32_with_context.clone()
                    } else {
                        // å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªè¯­éŸ³æ®µï¼Œåˆå¹¶æ‰€æœ‰æ®µ
                        // è¿™æ ·å¯ä»¥å¤„ç†åŒ…å«å¤šä¸ªå¥å­çš„é•¿éŸ³é¢‘
                        let mut processed_audio = Vec::new();
                        
                        for (start, end) in &segments {
                            let segment = &audio_f32_with_context[*start..*end];
                            processed_audio.extend_from_slice(segment);
                        }
                        
                        info!(
                            trace_id = %trace_id,
                            segments_count = segments.len(),
                            original_samples = audio_f32_with_context.len(),
                            processed_samples = processed_audio.len(),
                            removed_samples = audio_f32_with_context.len() - processed_audio.len(),
                            "VADæ£€æµ‹åˆ°{}ä¸ªè¯­éŸ³æ®µï¼Œå·²æå–æœ‰æ•ˆè¯­éŸ³", segments.len()
                        );
                        
                        // å¦‚æœå¤„ç†åçš„éŸ³é¢‘å¤ªçŸ­ï¼ˆ< 0.5ç§’ï¼‰ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘
                        const MIN_AUDIO_SAMPLES: usize = 8000; // 0.5ç§’ @ 16kHz
                        if processed_audio.len() < MIN_AUDIO_SAMPLES {
                            warn!(
                                trace_id = %trace_id,
                                processed_samples = processed_audio.len(),
                                "VADå¤„ç†åçš„éŸ³é¢‘è¿‡çŸ­ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘"
                            );
                            audio_f32_with_context.clone()
                        } else {
                            processed_audio
                        }
                    }
                }
                Err(e) => {
                    warn!(
                        trace_id = %trace_id,
                        error = %e,
                        "VADæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å®Œæ•´éŸ³é¢‘è¿›è¡ŒASR"
                    );
                    audio_f32_with_context.clone()
                }
            }
        };
        
        // å¦‚æœå¯ç”¨äº†æµå¼ ASRï¼Œä½¿ç”¨æµå¼å¤„ç†ï¼›å¦åˆ™ä½¿ç”¨ä¸€æ¬¡æ€§å¤„ç†
        let transcript = if request.enable_streaming_asr.unwrap_or(false) {
            // å¯ç”¨æµå¼ ASR
            let interval_ms = request.partial_update_interval_ms.unwrap_or(1000);
            self.asr_engine.enable_streaming(interval_ms).await;
            
            // è®¾ç½®è¯­è¨€ï¼ˆéœ€è¦åœ¨æµå¼å¤„ç†å‰è®¾ç½®ï¼‰
            // æ³¨æ„ï¼šASREngine çš„ set_language éœ€è¦ &mutï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å†…éƒ¨å¯å˜æ€§
            // ç”±äº ASREngine å†…éƒ¨ä½¿ç”¨ Arcï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹è®¾è®¡æˆ–ä½¿ç”¨å…¶ä»–æ–¹å¼
            // æš‚æ—¶åœ¨æµå¼å¤„ç†ä¸­ï¼Œè¯­è¨€ä¼šåœ¨ get_partial_result å’Œ get_final_result ä¸­ä½¿ç”¨ self.language
            
            // å°†éŸ³é¢‘æ•°æ®åˆ†å—å¤„ç†ï¼ˆæ¨¡æ‹Ÿæµå¼è¾“å…¥ï¼‰
            // æ¯å—çº¦ 0.5 ç§’ï¼ˆ8000 ä¸ªæ ·æœ¬ @ 16kHzï¼‰
            let chunk_size = 8000;
            let mut current_timestamp_ms = 0u64;
            let sample_rate = 16000u32;
            let chunk_duration_ms = (chunk_size * 1000) / sample_rate;
            
            // æ¸…ç©ºç¼“å†²åŒº
            self.asr_engine.clear_buffer().await;
            
            // åˆ†å—ç´¯ç§¯éŸ³é¢‘å¹¶å®šæœŸè·å–éƒ¨åˆ†ç»“æœ
            for chunk in audio_f32_processed.chunks(chunk_size as usize) {
                // ç´¯ç§¯éŸ³é¢‘å—
                self.asr_engine.accumulate_audio(chunk).await;
                
                // æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºéƒ¨åˆ†ç»“æœ
                if let Some(partial) = self.asr_engine.get_partial_result(current_timestamp_ms, &src_lang).await? {
                    // é€šè¿‡å›è°ƒå‘é€éƒ¨åˆ†ç»“æœ
                    if let Some(ref callback) = partial_callback {
                        callback(partial.clone());
                    }
                }
                
                current_timestamp_ms += chunk_duration_ms as u64;
            }
            
            // è·å–æœ€ç»ˆç»“æœ
            let final_text = self.asr_engine.get_final_result(&src_lang).await?;
            // ç¦ç”¨æµå¼æ¨¡å¼
            self.asr_engine.disable_streaming().await;
            final_text
        } else {
            // ä¸€æ¬¡æ€§å¤„ç†ï¼ˆä½¿ç”¨VADå¤„ç†åçš„éŸ³é¢‘ï¼‰
            self.asr_engine.transcribe_f32(&audio_f32_processed, &src_lang).await?
        };
        
        // å°† ASR ç»“æœå†™å…¥ PipelineContext
        // è®°å½•è¿‡æ»¤å‰åçš„æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if transcript.contains('(') || transcript.contains('ï¼ˆ') || transcript.contains('[') || transcript.contains('ã€') {
            warn!(
                trace_id = %trace_id,
                transcript = %transcript,
                transcript_len = transcript.len(),
                "âš ï¸ [ASR Filter Check] Transcript contains brackets before setting to context!"
            );
        }
        ctx.set_transcript(transcript.clone());
        info!(
            trace_id = %trace_id,
            transcript_len = transcript.len(),
            transcript_preview = %transcript.chars().take(50).collect::<String>(),
            transcript_trimmed_len = transcript.trim().len(),
            "âœ… ASR è¯†åˆ«å®Œæˆ"
        );

        // 3. å¯é€‰æ¨¡å—å¤„ç†ï¼ˆä½¿ç”¨ PipelineContextï¼‰
        // 3.1 éŸ³è‰²è¯†åˆ«ï¼ˆä½¿ç”¨ VAD å¤„ç†åçš„éŸ³é¢‘æ•°æ®ï¼Œå»é™¤é™éŸ³éƒ¨åˆ†ï¼Œæé«˜è¯†åˆ«å‡†ç¡®æ€§ï¼‰
        if features.map(|f| f.speaker_identification).unwrap_or(false) {
            if let Some(ref m) = self.speaker_identifier {
                let module = m.read().await;
                if InferenceModule::is_enabled(&*module) {
                    match module.identify(&audio_f32_processed).await {
                        Ok(result) => {
                            ctx.set_speaker_id(result.speaker_id.clone());
                            // å¦‚æœè¿”å›äº† voice_embeddingï¼Œå¯ä»¥ä¿å­˜åˆ° PipelineContextï¼ˆå¦‚æœéœ€è¦ï¼‰
                            if let Some(ref embedding) = result.voice_embedding {
                                info!(trace_id = %trace_id, speaker_id = %result.speaker_id, embedding_dim = embedding.len(), "è¯´è¯è€…è¯†åˆ«å®Œæˆ");
                            } else {
                                info!(trace_id = %trace_id, speaker_id = %result.speaker_id, "è¯´è¯è€…è¯†åˆ«å®Œæˆï¼ˆæ—  embeddingï¼‰");
                            }
                        }
                        Err(e) => {
                            warn!(trace_id = %trace_id, error = %e, "è¯´è¯è€…è¯†åˆ«å¤±è´¥");
                        }
                    }
                }
            }
        }

        // 3.2 è¯­é€Ÿè¯†åˆ«
        if features.map(|f| f.speech_rate_detection).unwrap_or(false) {
            // ä¼°ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆç®€åŒ–å®ç°ï¼‰
            let duration = request.audio_data.len() as f32 / 16000.0 / 2.0; // å‡è®¾ 16kHz, 16bit
            if let Some(ref m) = self.speech_rate_detector {
                let module = m.read().await;
                if InferenceModule::is_enabled(&*module) {
                    if let Ok(rate) = module.detect(&request.audio_data, duration).await {
                        ctx.set_speech_rate(rate);
                    }
                }
            }
        }

        // 3.3 æƒ…æ„Ÿæ£€æµ‹ï¼ˆéœ€è¦ transcriptï¼‰
        if features.map(|f| f.emotion_detection).unwrap_or(false) {
            // TODO: å®ç°æƒ…æ„Ÿæ£€æµ‹æ¨¡å—
            // å½“å‰å…ˆè·³è¿‡ï¼Œå¾…å®ç° emotion_detection æ¨¡å—
        }

        // 3.4 ä¸ªæ€§åŒ–é€‚é…ï¼ˆéœ€è¦ transcriptï¼‰
        if features.map(|f| f.persona_adaptation).unwrap_or(false) {
            // TODO: å®ç°ä¸ªæ€§åŒ–é€‚é…æ¨¡å—
            // å½“å‰å…ˆè·³è¿‡ï¼Œå¾…å®ç° persona_adaptation æ¨¡å—
        }

        // 4. NMT: æœºå™¨ç¿»è¯‘ï¼ˆå¿…éœ€ï¼Œä½¿ç”¨åŠ¨æ€ç¡®å®šçš„ç¿»è¯‘æ–¹å‘ï¼‰
        // å¦‚æœ ASR ç»“æœä¸ºç©ºæˆ–æ— æ„ä¹‰ï¼Œè·³è¿‡ç¿»è¯‘å’Œ TTSï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
        // è¿™æ ·å¯ä»¥é¿å…å¯¹é™éŸ³è¯†åˆ«ç»“æœè¿›è¡Œç¿»è¯‘å’ŒTTSï¼ŒèŠ‚çœèµ„æº
        // é‡è¦ï¼šåœ¨æ£€æŸ¥ç©ºæ–‡æœ¬åï¼Œæ‰æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼Œé¿å…é™éŸ³éŸ³é¢‘æ±¡æŸ“ä¸Šä¸‹æ–‡
        let transcript_trimmed = transcript.trim();
        if transcript_trimmed.is_empty() {
            warn!(
                trace_id = %trace_id,
                transcript = %transcript,
                "ASR transcript is empty, skipping NMT and TTS, and NOT updating context buffer"
            );
            // è¿”å›ç©ºç»“æœï¼Œä¸è¿›è¡Œç¿»è¯‘å’Œ TTSï¼Œä¹Ÿä¸æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒº
            let result = InferenceResult {
                transcript: String::new(),
                translation: String::new(),
                audio: Vec::new(),
                speaker_id: None,
                speech_rate: None,
                emotion: None,
            };
            return Ok(result);
        }
        
        // æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºæ— æ„ä¹‰çš„è¯†åˆ«ç»“æœï¼ˆå¦‚é™éŸ³æ—¶çš„è¯¯è¯†åˆ«ï¼‰
        if crate::text_filter::is_meaningless_transcript(transcript_trimmed) {
            warn!(
                trace_id = %trace_id,
                transcript = %transcript_trimmed,
                transcript_len = transcript_trimmed.len(),
                "ASR transcript is meaningless (likely silence misrecognition), skipping NMT and TTS, and NOT updating context buffer"
            );
            // è¿”å›ç©ºç»“æœï¼Œä¸è¿›è¡Œç¿»è¯‘å’Œ TTSï¼Œä¹Ÿä¸æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒº
            let result = InferenceResult {
                transcript: String::new(),
                translation: String::new(),
                audio: Vec::new(),
                speaker_id: None,
                speech_rate: None,
                emotion: None,
            };
            return Ok(result);
        }
        
        // 2.1 æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼šä½¿ç”¨VADé€‰æ‹©æœ€ä½³ä¸Šä¸‹æ–‡ç‰‡æ®µ
        // ä¼˜å…ˆé€‰æ‹©æœ€åä¸€ä¸ªè¯­éŸ³æ®µçš„å°¾éƒ¨ï¼Œè€Œä¸æ˜¯ç®€å•çš„éŸ³é¢‘å°¾éƒ¨
        // é‡è¦ï¼šåªæœ‰åœ¨æ–‡æœ¬æœ‰æ„ä¹‰æ—¶æ‰æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼Œé¿å…é™éŸ³éŸ³é¢‘æ±¡æŸ“ä¸Šä¸‹æ–‡
        {
            const CONTEXT_DURATION_SEC: f32 = 2.0;  // ä¿å­˜æœ€å2ç§’
            const SAMPLE_RATE: u32 = 16000;
            let context_samples = (CONTEXT_DURATION_SEC * SAMPLE_RATE as f32) as usize;
            
            let mut context = self.context_buffer.lock().await;
            
            // ä½¿ç”¨VADæ£€æµ‹åŸå§‹éŸ³é¢‘ï¼ˆä¸å¸¦ä¸Šä¸‹æ–‡ï¼‰çš„è¯­éŸ³æ®µ
            match self.vad_engine.detect_speech(&audio_f32) {
                Ok(segments) => {
                    if !segments.is_empty() {
                        // é€‰æ‹©æœ€åä¸€ä¸ªè¯­éŸ³æ®µ
                        let (last_start, last_end) = segments.last().unwrap();
                        let last_segment = &audio_f32[*last_start..*last_end];
                        
                        // ä»æœ€åä¸€ä¸ªè¯­éŸ³æ®µçš„å°¾éƒ¨æå–ä¸Šä¸‹æ–‡
                        if last_segment.len() > context_samples {
                            let start_idx = last_segment.len() - context_samples;
                            *context = last_segment[start_idx..].to_vec();
                            info!(
                                trace_id = %trace_id,
                                context_samples = context.len(),
                                context_duration_sec = (context.len() as f32 / 16000.0),
                                segment_start = last_start,
                                segment_end = last_end,
                                segment_samples = last_segment.len(),
                                "âœ… æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼ˆä½¿ç”¨VADé€‰æ‹©çš„æœ€åä¸€ä¸ªè¯­éŸ³æ®µå°¾éƒ¨ï¼‰"
                            );
                        } else {
                            // å¦‚æœæœ€åä¸€ä¸ªæ®µå¤ªçŸ­ï¼Œä¿å­˜æ•´ä¸ªæ®µ
                            *context = last_segment.to_vec();
                            info!(
                                trace_id = %trace_id,
                                context_samples = context.len(),
                                context_duration_sec = (context.len() as f32 / 16000.0),
                                segment_samples = last_segment.len(),
                                "âœ… æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼ˆæœ€åä¸€ä¸ªè¯­éŸ³æ®µè¾ƒçŸ­ï¼Œä¿å­˜å…¨éƒ¨ï¼‰"
                            );
                        }
                    } else {
                        // å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œå›é€€åˆ°ç®€å•å°¾éƒ¨ä¿å­˜
                        if audio_f32.len() > context_samples {
                            let start_idx = audio_f32.len() - context_samples;
                            *context = audio_f32[start_idx..].to_vec();
                            info!(
                                trace_id = %trace_id,
                                context_samples = context.len(),
                                context_duration_sec = (context.len() as f32 / 16000.0),
                                original_samples = audio_f32.len(),
                                "âš ï¸ æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼ˆVADæœªæ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œä¿å­˜æœ€å{}ç§’ï¼‰", CONTEXT_DURATION_SEC
                            );
                        } else {
                            *context = audio_f32.clone();
                            info!(
                                trace_id = %trace_id,
                                context_samples = context.len(),
                                context_duration_sec = (context.len() as f32 / 16000.0),
                                original_samples = audio_f32.len(),
                                "âš ï¸ æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼ˆutteranceè¾ƒçŸ­ï¼Œä¿å­˜å…¨éƒ¨ï¼‰"
                            );
                        }
                    }
                }
                Err(e) => {
                    // VADæ£€æµ‹å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•å°¾éƒ¨ä¿å­˜
                    warn!(
                        trace_id = %trace_id,
                        error = %e,
                        "VADæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•å°¾éƒ¨ä¿å­˜ä¸Šä¸‹æ–‡"
                    );
                    if audio_f32.len() > context_samples {
                        let start_idx = audio_f32.len() - context_samples;
                        *context = audio_f32[start_idx..].to_vec();
                        info!(
                            trace_id = %trace_id,
                            context_samples = context.len(),
                            context_duration_sec = (context.len() as f32 / 16000.0),
                            "âš ï¸ æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼ˆVADå¤±è´¥å›é€€ï¼Œä¿å­˜æœ€å{}ç§’ï¼‰", CONTEXT_DURATION_SEC
                        );
                    } else {
                        *context = audio_f32.clone();
                        info!(
                            trace_id = %trace_id,
                            context_samples = context.len(),
                            context_duration_sec = (context.len() as f32 / 16000.0),
                            "âš ï¸ æ›´æ–°ä¸Šä¸‹æ–‡ç¼“å†²åŒºï¼ˆVADå¤±è´¥å›é€€ï¼Œutteranceè¾ƒçŸ­ï¼Œä¿å­˜å…¨éƒ¨ï¼‰"
                        );
                    }
                }
            }
        }
        
        debug!(trace_id = %trace_id, src_lang = %src_lang, tgt_lang = %tgt_lang, "å¼€å§‹æœºå™¨ç¿»è¯‘");
        let context_text = request.context_text.as_deref();
        let translation = self.nmt_engine.translate(&transcript, &src_lang, &tgt_lang, context_text).await?;
        
        // å°†ç¿»è¯‘ç»“æœå†™å…¥ PipelineContext
        ctx.set_translation(translation.clone());
        info!(trace_id = %trace_id, translation_len = translation.len(), "æœºå™¨ç¿»è¯‘å®Œæˆ");

        // 5. TTS: è¯­éŸ³åˆæˆï¼ˆå¿…éœ€ï¼Œä½¿ç”¨ç›®æ ‡è¯­è¨€ï¼‰
        // æ ¹æ® features.voice_cloning é€‰æ‹©ä½¿ç”¨ YourTTS æˆ– Piper TTS
        debug!(trace_id = %trace_id, tgt_lang = %tgt_lang, "å¼€å§‹è¯­éŸ³åˆæˆ");
        let use_voice_cloning = features.map(|f| f.voice_cloning).unwrap_or(false);
        let mut audio = if use_voice_cloning {
            // å¦‚æœå¯ç”¨éŸ³è‰²å…‹éš†ï¼Œå°è¯•ä½¿ç”¨ YourTTS
            if let Some(ref speaker_id) = ctx.speaker_id {
                if let Some(ref cloner) = self.voice_cloner {
                    let module = cloner.read().await;
                    if InferenceModule::is_enabled(&*module) {
                        match module.clone_voice(&translation, speaker_id, Some(&tgt_lang)).await {
                            Ok(cloned_audio) => {
                                info!(trace_id = %trace_id, speaker_id = %speaker_id, "ä½¿ç”¨ YourTTS è¿›è¡ŒéŸ³è‰²å…‹éš†");
                                cloned_audio
                            }
                            Err(e) => {
                                warn!(trace_id = %trace_id, error = %e, "YourTTS éŸ³è‰²å…‹éš†å¤±è´¥ï¼Œé™çº§åˆ° Piper TTS");
                                // é™çº§åˆ° Piper TTS
                                self.tts_engine.synthesize(&translation, &tgt_lang).await?
                            }
                        }
                    } else {
                        // æ¨¡å—æœªå¯ç”¨ï¼Œä½¿ç”¨ Piper TTS
                        warn!(trace_id = %trace_id, "Voice cloning module not enabled, using Piper TTS");
                        self.tts_engine.synthesize(&translation, &tgt_lang).await?
                    }
                } else {
                    // VoiceCloner æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ Piper TTS
                    warn!(trace_id = %trace_id, "VoiceCloner not initialized, using Piper TTS");
                    self.tts_engine.synthesize(&translation, &tgt_lang).await?
                }
            } else {
                // æ²¡æœ‰ speaker_idï¼Œä½¿ç”¨ Piper TTS
                warn!(trace_id = %trace_id, "No speaker_id available, using Piper TTS");
                self.tts_engine.synthesize(&translation, &tgt_lang).await?
            }
        } else {
            // æ ‡å‡†æµç¨‹ï¼Œä½¿ç”¨ Piper TTS
            self.tts_engine.synthesize(&translation, &tgt_lang).await?
        };
        info!(trace_id = %trace_id, audio_len = audio.len(), "è¯­éŸ³åˆæˆå®Œæˆ");

        // 6. å¯é€‰ï¼šè¯­é€Ÿæ§åˆ¶ï¼ˆéœ€è¦è¯­é€Ÿè¯†åˆ«ç»“æœï¼‰
        if features.map(|f| f.speech_rate_control).unwrap_or(false) {
            if let Some(rate) = ctx.speech_rate {
                if let Some(ref controller) = self.speech_rate_controller {
                    let module = controller.read().await;
                    if InferenceModule::is_enabled(&*module) {
                        // å‡è®¾ç›®æ ‡è¯­é€Ÿä¸º 1.0ï¼ˆæ­£å¸¸é€Ÿåº¦ï¼‰
                        let target_rate = 1.0;
                        if let Ok(adjusted) = module.adjust_audio(&audio, target_rate, rate) {
                            audio = adjusted;
                        }
                    }
                }
            }
        }
        
        // å°†æœ€ç»ˆ TTS éŸ³é¢‘å†™å…¥ PipelineContext
        ctx.set_tts_audio(audio.clone());

        // ä» PipelineContext æ„å»º InferenceResult
        info!(trace_id = %trace_id, job_id = %request.job_id, "æ¨ç†è¯·æ±‚å¤„ç†å®Œæˆ");
        Ok(InferenceResult {
            transcript: ctx.transcript.unwrap_or_default(),
            translation: ctx.translation.unwrap_or_default(),
            audio,
            speaker_id: ctx.speaker_id,
            speech_rate: ctx.speech_rate,
            emotion: ctx.emotion,
        })
    }
}

