# 语义修复服务备选方案（Plan B / C / D）
**版本**：v1.0  
**适用对象**：开发部门 / 节点端架构评审  
**前提条件**：  
- 语义修复已设计为**独立服务**  
- 不使用 CPU / FP16  
- 目标为 **INT4 GPU 推理、低并发、低系统开销**  

---

## 1. 背景说明（统一认知）

当前语义修复服务已满足：
- 服务级隔离（独立进程 / HTTP 或 IPC）
- 可丢弃（PASS）设计，不阻塞主链路
- 低并发、节点端（家用 PC）运行环境

因此，后续讨论的“Plan B / C / D”**不再涉及是否服务化**，而是围绕以下三个维度展开：

1. **推理引擎选择**
2. **承载方式（绑定 / 可执行）**
3. **运行时与依赖隔离方式**

---

## 2. Plan B —— 同引擎、换承载方式（优先级最高）

### 2.1 定义
在保持 **llama.cpp + GGUF INT4** 技术路线不变的前提下：
- 放弃 `llama-cpp-python` 绑定
- 改为使用 **llama.cpp 官方 `llama-server` 可执行程序**
- 通过 HTTP 调用进行语义修复推理

### 2.2 适用场景
- Python 绑定层编译困难或不稳定
- Windows 节点端存在 CMake / CUDA / Python ABI 问题
- 希望最小化运行时依赖

### 2.3 优点
- 完全绕开 Python 绑定与编译链
- 依赖最少（exe + GGUF 模型）
- 系统开销最低
- 与当前“独立服务 + PASS”设计完全兼容

### 2.4 风险与限制
- 需要维护 HTTP 接口适配层
- 参数映射需与现有 repair 接口保持一致

### 2.5 结论
**Plan B 是 llama.cpp 路线下的首选兜底方案。**

---

## 3. Plan C —— 更换推理引擎（INT4 GPU）

### 3.1 定义
当 llama.cpp（无论 Python 绑定或 llama-server）在特定硬件/驱动组合上不可用或表现不稳定时：
- 切换至 **ExLlamaV2（EXL2 4bit）**
- 保持 INT4 GPU 推理目标不变

### 3.2 适用场景
- llama.cpp GPU backend 无法稳定工作
- GGUF 推理在特定 GPU 上存在异常
- 需要另一条成熟的 INT4 GPU 推理路径

### 3.3 优点
- EXL2 4bit 推理效率高
- 不依赖 auto-gptq
- 与低并发、短文本场景匹配

### 3.4 风险与限制
- 工程依赖高于 llama.cpp
- 模型格式不同（EXL2 ≠ GGUF）
- 需要维护第二套模型文件

### 3.5 结论
**Plan C 是 INT4 GPU 层面的“引擎级备选”。**

---

## 4. Plan D —— 运行时与依赖隔离（虚拟环境）

### 4.1 定义（澄清）
Plan D **不是重新服务化**，而是：
> 在语义修复已是独立服务的前提下，  
> 进一步实现 **运行时/依赖隔离**。

具体做法包括：
- 独立 Python 虚拟环境（venv / conda）
- 独立目录结构，不共享 site-packages
- 启动脚本显式绑定虚拟环境

### 4.2 适用场景
- 节点端同机运行多个 Python 服务
- CUDA / PyTorch / 推理库依赖存在冲突风险
- Windows 家用 PC 环境不可控

### 4.3 优点
- 显著降低依赖冲突概率
- 不影响主系统 Python 环境
- 与 Plan B / Plan C **正交可组合**

### 4.4 推荐实践
- llama-server：无需 Python，天然满足 Plan D
- ExLlamaV2：**必须**放入独立 venv
- 明确禁止与 ASR / NMT / 其他服务共用 Python 环境

### 4.5 结论
**Plan D 是稳定性增强方案，而非功能备选方案。**

---

## 5. 方案组合关系（重要）

| 场景 | 推荐组合 |
|---|---|
| llama-cpp-python 不稳定 | Plan B |
| llama.cpp 整体不可用 | Plan C |
| 节点端依赖复杂 | Plan D |
| 推荐默认 | **Plan B + Plan D** |
| 最强兜底 | **Plan C + Plan D** |

---

## 6. 推荐决策顺序（给开发部门）

1. **优先落地 llama.cpp（GGUF INT4）**
2. 若 Python 绑定不稳 → 切换 **llama-server（Plan B）**
3. 若 llama.cpp GPU 不可用 → 切换 **ExLlamaV2（Plan C）**
4. 所有方案统一启用 **Plan D（独立虚拟环境）**

---

## 7. 最终结论

在不引入 CPU / FP16、不回退 PyTorch 的前提下：
- Plan B / C 解决的是 **“跑不跑得通”**
- Plan D 解决的是 **“跑得稳不稳”**

该三方案组合可以覆盖节点端绝大多数不可控风险，
且与当前语义修复架构 **完全兼容、无需重构主系统**。

---

**本文件可直接作为开发部门的技术决策与实施参考。**
