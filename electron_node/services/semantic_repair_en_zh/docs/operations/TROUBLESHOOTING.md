# æ•…éšœæ’æŸ¥æŒ‡å—

**æœåŠ¡**: semantic-repair-en-zh  
**ç‰ˆæœ¬**: 1.0.0

---

## ğŸ“‹ ç›®å½•

- [å¸¸è§é—®é¢˜å¿«é€Ÿç´¢å¼•](#å¸¸è§é—®é¢˜å¿«é€Ÿç´¢å¼•)
- [æœåŠ¡å¯åŠ¨é—®é¢˜](#æœåŠ¡å¯åŠ¨é—®é¢˜)
- [GPU æ”¯æŒé—®é¢˜](#gpu-æ”¯æŒé—®é¢˜)
- [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
- [æ¨¡å‹åŠ è½½é—®é¢˜](#æ¨¡å‹åŠ è½½é—®é¢˜)
- [API è°ƒç”¨é—®é¢˜](#api-è°ƒç”¨é—®é¢˜)
- [è¯Šæ–­å·¥å…·](#è¯Šæ–­å·¥å…·)

---

## ğŸ” å¸¸è§é—®é¢˜å¿«é€Ÿç´¢å¼•

| é—®é¢˜ç—‡çŠ¶ | å¯èƒ½åŸå›  | ç« èŠ‚é“¾æ¥ |
|---------|---------|---------|
| æœåŠ¡æ— æ³•å¯åŠ¨ | æ¨¡å‹æœªæ‰¾åˆ°ã€ç«¯å£å ç”¨ã€ä¾èµ–ç¼ºå¤± | [æœåŠ¡å¯åŠ¨é—®é¢˜](#æœåŠ¡å¯åŠ¨é—®é¢˜) |
| GPU æœªè¢«ä½¿ç”¨ | llama-cpp-python æ—  CUDA æ”¯æŒ | [GPU æ”¯æŒé—®é¢˜](#gpu-æ”¯æŒé—®é¢˜) |
| å“åº”é€Ÿåº¦æ…¢ | GPU æœªå¯ç”¨ã€CPU æ¨¡å¼ | [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜) |
| æ¨¡å‹åŠ è½½å¤±è´¥ | æ–‡ä»¶æŸåã€è·¯å¾„é”™è¯¯ | [æ¨¡å‹åŠ è½½é—®é¢˜](#æ¨¡å‹åŠ è½½é—®é¢˜) |
| API è¿”å› 503 | å¤„ç†å™¨æœªåˆå§‹åŒ– | [API è°ƒç”¨é—®é¢˜](#api-è°ƒç”¨é—®é¢˜) |

---

## ğŸš€ æœåŠ¡å¯åŠ¨é—®é¢˜

### é—®é¢˜ 1: æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°

**é”™è¯¯ä¿¡æ¯**:
```
[Config] WARNING: zh model not found at: .../models/qwen2.5-3b-instruct-zh-gguf
[Config] Please copy model to: .../models/qwen2.5-3b-instruct-zh-gguf
```

**åŸå› **: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨

**è§£å†³æ–¹æ¡ˆ**:
```powershell
# è¿è¡Œæ¨¡å‹å®‰è£…è„šæœ¬
cd semantic_repair_en_zh
.\setup_models.ps1

# æˆ–æ‰‹åŠ¨å¤åˆ¶
Copy-Item -Path "..\semantic_repair_zh\models\qwen2.5-3b-instruct-zh-gguf" -Destination "models\" -Recurse
Copy-Item -Path "..\semantic_repair_en\models\qwen2.5-3b-instruct-en-gguf" -Destination "models\" -Recurse
```

### é—®é¢˜ 2: ç«¯å£è¢«å ç”¨

**é”™è¯¯ä¿¡æ¯**:
```
OSError: [WinError 10048] é€šå¸¸æ¯ä¸ªå¥—æ¥å­—åœ°å€(åè®®/ç½‘ç»œåœ°å€/ç«¯å£)åªå…è®¸ä½¿ç”¨ä¸€æ¬¡
```

**åŸå› **: ç«¯å£ 5015 å·²è¢«å ç”¨

**æ£€æŸ¥ç«¯å£å ç”¨**:
```powershell
netstat -ano | findstr :5015
```

**è§£å†³æ–¹æ¡ˆ**:
```powershell
# æ–¹æ¡ˆ 1: åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹
$processId = (netstat -ano | findstr :5015 | ForEach-Object {$_.Trim() -split '\s+'} | Select-Object -Last 1)
Stop-Process -Id $processId -Force

# æ–¹æ¡ˆ 2: ä¿®æ”¹ç«¯å£ï¼ˆä¸æ¨èï¼‰
$env:PORT=5016
python service.py
```

### é—®é¢˜ 3: Python ä¾èµ–ç¼ºå¤±

**é”™è¯¯ä¿¡æ¯**:
```
ModuleNotFoundError: No module named 'fastapi'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¦‚æœä½¿ç”¨ GPUï¼Œç¡®ä¿å®‰è£… CUDA ç‰ˆæœ¬çš„ llama-cpp-python
# å‚è€ƒ GPU æ”¯æŒé—®é¢˜ç« èŠ‚
```

---

## ğŸ® GPU æ”¯æŒé—®é¢˜

### æ¦‚è¿°

**GPU æ”¯æŒçš„é‡è¦æ€§**:
- CPU æ¨¡å¼: ~2000-4000ms/è¯·æ±‚
- GPU æ¨¡å¼: ~200-500ms/è¯·æ±‚
- **æ€§èƒ½æå‡ 5-10 å€**

### é—®é¢˜ 1: GPU æœªè¢«ä½¿ç”¨

**è¯Šæ–­æ–¹æ³•**:

1. **æ£€æŸ¥ PyTorch GPU æ”¯æŒ**:
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device: {torch.cuda.get_device_name(0)}")
```

2. **æ£€æŸ¥ llama-cpp-python GPU æ”¯æŒ**:
```python
from llama_cpp import Llama

# å¯åŠ¨æœåŠ¡å¹¶æŸ¥çœ‹æ—¥å¿—
# å¦‚æœçœ‹åˆ° "assigned to device CPU" åˆ™ GPU æœªå¯ç”¨
# åº”è¯¥çœ‹åˆ° "assigned to device CUDA"
```

3. **ç›‘æ§ GPU ä½¿ç”¨**:
```powershell
# å®æ—¶ç›‘æ§
nvidia-smi -l 1

# åœ¨æ¨ç†æ—¶åº”è¯¥çœ‹åˆ° GPU åˆ©ç”¨ç‡ä¸Šå‡
```

**ç—‡çŠ¶**:
- âŒ æ‰€æœ‰å±‚æ˜¾ç¤º `assigned to device CPU`
- âŒ nvidia-smi æ˜¾ç¤º GPU åˆ©ç”¨ç‡ä¸º 0%
- âŒ CPU ä½¿ç”¨ç‡æ¥è¿‘ 100%
- âŒ å“åº”æ—¶é—´ >2ç§’

**åŸå› **: llama-cpp-python å®‰è£…æ—¶æœªåŒ…å« CUDA æ”¯æŒ

**è§£å†³æ–¹æ¡ˆ A: ä½¿ç”¨é¢„ç¼–è¯‘ CUDA wheelï¼ˆæ¨èï¼Œæœ€å¿«ï¼‰**

```powershell
# å¸è½½ç°æœ‰ç‰ˆæœ¬
pip uninstall llama-cpp-python -y

# å®‰è£… CUDA ç‰ˆæœ¬
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

**æ³¨æ„**: æ›¿æ¢ `cu121` ä¸ºæ‚¨çš„ CUDA ç‰ˆæœ¬ï¼ˆ11.7â†’cu117, 11.8â†’cu118, 12.1â†’cu121 ç­‰ï¼‰

**è§£å†³æ–¹æ¡ˆ B: ä»æºç ç¼–è¯‘ï¼ˆæœ€å¯é ï¼Œä½†è€—æ—¶ï¼‰**

```powershell
# è®¾ç½®ç¼–è¯‘é€‰é¡¹
$env:CMAKE_ARGS="-DGGML_CUDA=on"
$env:FORCE_CMAKE=1

# ä»æºç å®‰è£…ï¼ˆéœ€è¦ 30-60 åˆ†é’Ÿï¼‰
pip uninstall llama-cpp-python -y
pip install llama-cpp-python --no-cache-dir --force-reinstall
```

**ç¼–è¯‘è¦æ±‚**:
- âœ… Visual Studio 2019/2022 (C++ å·¥å…·)
- âœ… CUDA Toolkit (ä¸ PyTorch ç‰ˆæœ¬åŒ¹é…)
- âœ… CMake
- âœ… è¶³å¤Ÿçš„æ—¶é—´ï¼ˆ30-60 åˆ†é’Ÿï¼‰

**è§£å†³æ–¹æ¡ˆ C: ä½¿ç”¨ condaï¼ˆç®€å•ä½†éœ€è¦ conda ç¯å¢ƒï¼‰**

```bash
conda install -c conda-forge llama-cpp-python
```

**éªŒè¯ GPU æ”¯æŒ**:

```bash
# å¯åŠ¨æœåŠ¡å¹¶æŸ¥çœ‹æ—¥å¿—
python service.py

# åº”è¯¥çœ‹åˆ°:
# [llama_model_load_internal] ggml_cuda_init: CUDA device 0: NVIDIA RTX 4060 Laptop GPU
# [load_tensors] layer 0 assigned to device CUDA
```

### é—®é¢˜ 2: CUDA ç‰ˆæœ¬ä¸åŒ¹é…

**é”™è¯¯ä¿¡æ¯**:
```
CUDA error: CUDA driver version is insufficient for CUDA runtime version
```

**è§£å†³æ–¹æ¡ˆ**:
```powershell
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvidia-smi  # æŸ¥çœ‹ Driver Version å’Œ CUDA Version

# æ£€æŸ¥ PyTorch CUDA ç‰ˆæœ¬
python -c "import torch; print(torch.version.cuda)"

# ç¡®ä¿ä¸¤è€…å…¼å®¹
```

### é—®é¢˜ 3: æ˜¾å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯**:
```
CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¿®æ”¹ config.pyï¼Œå‡å°‘ GPU å±‚æ•°
'n_gpu_layers': 20  # ä» -1 æ”¹ä¸ºå…·ä½“æ•°å€¼

# æˆ–å¼ºåˆ¶ä½¿ç”¨ CPU
'n_gpu_layers': 0
```

---

## âš¡ æ€§èƒ½é—®é¢˜

### é—®é¢˜ 1: å“åº”é€Ÿåº¦æ…¢ï¼ˆ>2ç§’ï¼‰

**è¯Šæ–­æ­¥éª¤**:

1. **æ£€æŸ¥ GPU ä½¿ç”¨**:
```powershell
nvidia-smi
```

2. **æµ‹è¯•å“åº”æ—¶é—´**:
```bash
time curl -X POST http://localhost:5015/zh/repair \
  -H "Content-Type: application/json" \
  -d '{"job_id":"test","session_id":"s1","text_in":"ä½ å¥½"}'
```

3. **æŸ¥çœ‹å¯åŠ¨æ—¥å¿—**:
```
# GPU æ¨¡å¼åº”è¯¥æ˜¾ç¤º:
[load_tensors] layer 0 assigned to device CUDA

# CPU æ¨¡å¼ä¼šæ˜¾ç¤º:
[load_tensors] layer 0 assigned to device CPU
```

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ**:

| åŸå›  | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| GPU æœªå¯ç”¨ | CPU 100%, GPU 0% | å‚è€ƒ [GPU æ”¯æŒé—®é¢˜](#gpu-æ”¯æŒé—®é¢˜) |
| å¹¶å‘è¯·æ±‚ | å¤šä¸ªè¯·æ±‚æ’é˜Ÿ | æ£€æŸ¥ max_concurrency é…ç½® |
| æ¨¡å‹è¿‡å¤§ | æ˜¾å­˜ä¸è¶³ | å‡å°‘ n_gpu_layers |
| ç½‘ç»œå»¶è¿Ÿ | API è°ƒç”¨æ…¢ | æ£€æŸ¥ç½‘ç»œè¿æ¥ |

### é—®é¢˜ 2: é¦–æ¬¡è¯·æ±‚è¶…æ—¶

**ç—‡çŠ¶**: ç¬¬ä¸€ä¸ªè¯·æ±‚ç­‰å¾… 30+ ç§’

**åŸå› **: æ¨¡å‹åŠ è½½æ—¶é—´ï¼ˆæ­£å¸¸ç°è±¡ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```python
# æœåŠ¡å¯åŠ¨æ—¶é¢„çƒ­ï¼ˆåœ¨ lifespan ä¸­ï¼‰
# å·²åœ¨ä»£ç ä¸­å®ç°ï¼Œä¸éœ€è¦é¢å¤–é…ç½®
```

### é—®é¢˜ 3: å†…å­˜ä½¿ç”¨è¿‡é«˜

**ç—‡çŠ¶**: æœåŠ¡å ç”¨å†…å­˜æŒç»­å¢é•¿

**è¯Šæ–­**:
```python
import psutil
import os

process = psutil.Process(os.getpid())
print(f"Memory: {process.memory_info().rss / 1024 / 1024:.2f} MB")
```

**è§£å†³æ–¹æ¡ˆ**:
- å®šæœŸé‡å¯æœåŠ¡
- æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼
- è°ƒæ•´ n_ctx å‚æ•°ï¼ˆå‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ï¼‰

---

## ğŸ—„ï¸ æ¨¡å‹åŠ è½½é—®é¢˜

### é—®é¢˜ 1: æ¨¡å‹æ–‡ä»¶æŸå

**é”™è¯¯ä¿¡æ¯**:
```
ggml_init_from_file: failed to load model
```

**è§£å†³æ–¹æ¡ˆ**:
```powershell
# ä»å¤‡ä»½æ¢å¤
Copy-Item -Path "models.backup\*" -Destination "models\" -Recurse -Force

# æˆ–é‡æ–°ä¸‹è½½æ¨¡å‹
.\setup_models.ps1
```

### é—®é¢˜ 2: æ¨¡å‹æ ¼å¼ä¸å…¼å®¹

**é”™è¯¯ä¿¡æ¯**:
```
invalid model file (bad magic)
```

**åŸå› **: GGUF æ ¼å¼ç‰ˆæœ¬ä¸å…¼å®¹

**è§£å†³æ–¹æ¡ˆ**:
- å‡çº§ llama-cpp-python
- æˆ–ä½¿ç”¨å…¼å®¹çš„æ¨¡å‹ç‰ˆæœ¬

### é—®é¢˜ 3: æ¨¡å‹åŠ è½½è¶…æ—¶

**ç—‡çŠ¶**: æœåŠ¡å¯åŠ¨æ—¶å¡ä½

**åŸå› **: 
- æ¨¡å‹æ–‡ä»¶è¿‡å¤§
- ç£ç›˜ I/O æ…¢
- GPU åˆå§‹åŒ–æ…¢

**è§£å†³æ–¹æ¡ˆ**:
- è€å¿ƒç­‰å¾…ï¼ˆé¦–æ¬¡åŠ è½½éœ€è¦æ—¶é—´ï¼‰
- æ£€æŸ¥ç£ç›˜æ€§èƒ½
- æŸ¥çœ‹å¯åŠ¨æ—¥å¿—ç¡®è®¤è¿›åº¦

---

## ğŸŒ API è°ƒç”¨é—®é¢˜

### é—®é¢˜ 1: 503 Service Unavailable

**é”™è¯¯å“åº”**:
```json
{
  "detail": "Processor 'zh_repair' not available"
}
```

**åŸå› **: å¤„ç†å™¨æœªåˆå§‹åŒ–æˆ–åˆå§‹åŒ–å¤±è´¥

**è¯Šæ–­**:
```bash
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:5015/health

# æŸ¥çœ‹å„å¤„ç†å™¨çŠ¶æ€
curl http://localhost:5015/zh/health
```

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
- æŸ¥çœ‹æœåŠ¡å¯åŠ¨æ—¥å¿—
- é‡å¯æœåŠ¡

### é—®é¢˜ 2: è¯·æ±‚è¶…æ—¶

**é”™è¯¯**: è¯·æ±‚ç­‰å¾… 30+ ç§’åè¶…æ—¶

**åŸå› **: 
- å¤„ç†å™¨å¤„ç†æ—¶é—´è¿‡é•¿
- GPU æœªå¯ç”¨ï¼ˆCPU æ¨¡å¼æ…¢ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```python
# è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆconfig.pyï¼‰
self.timeout = 60  # ä» 30 æ”¹ä¸º 60 ç§’

# æˆ–å¯ç”¨ GPU åŠ é€Ÿ
```

### é—®é¢˜ 3: 422 Validation Error

**é”™è¯¯å“åº”**:
```json
{
  "detail": [
    {
      "loc": ["body", "job_id"],
      "msg": "field required",
      "type": "value_error.missing"
    }
  ]
}
```

**åŸå› **: è¯·æ±‚å‚æ•°ç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥è¯·æ±‚æ ¼å¼ï¼Œå‚è€ƒ [API å‚è€ƒ](./API_REFERENCE.md)

---

## ğŸ”§ è¯Šæ–­å·¥å…·

### 1. è¯­æ³•æ£€æŸ¥

```bash
cd semantic_repair_en_zh
python check_syntax.py
```

### 2. å•å…ƒæµ‹è¯•

```bash
pytest tests/ -v
```

### 3. å¥åº·æ£€æŸ¥

```bash
# å…¨å±€å¥åº·
curl http://localhost:5015/health

# å„å¤„ç†å™¨å¥åº·
curl http://localhost:5015/zh/health
curl http://localhost:5015/en/health
```

### 4. GPU ç›‘æ§

```powershell
# å®æ—¶ç›‘æ§
nvidia-smi -l 1

# è¯¦ç»†ä¿¡æ¯
nvidia-smi --query-gpu=name,temperature.gpu,utilization.gpu,utilization.memory,memory.used --format=csv -lms 1000
```

### 5. æ€§èƒ½æµ‹è¯•

```bash
# æµ‹è¯•å“åº”æ—¶é—´
for i in {1..10}; do
  time curl -X POST http://localhost:5015/zh/repair \
    -H "Content-Type: application/json" \
    -d "{\"job_id\":\"perf-$i\",\"session_id\":\"s1\",\"text_in\":\"æµ‹è¯•\"}"
done
```

---

## ğŸ“ æ—¥å¿—åˆ†æ

### å…³é”®æ—¥å¿—æ¨¡å¼

**æ­£å¸¸å¯åŠ¨**:
```
[Unified SR] ===== Starting Unified Semantic Repair Service =====
[Config] Found zh model: .../models/qwen2.5-3b-instruct-zh-gguf/*.gguf
[Config] Found en model: .../models/qwen2.5-3b-instruct-en-gguf/*.gguf
[zh_repair] Loading Chinese model...
[zh_repair] Model warmed up successfully
[Unified SR] Service ready with 3 processor(s)
```

**GPU å·²å¯ç”¨**:
```
ggml_cuda_init: CUDA device 0: NVIDIA RTX 4060 Laptop GPU
load_tensors: layer 0 assigned to device CUDA
```

**GPU æœªå¯ç”¨**:
```
load_tensors: layer 0 assigned to device CPU  # â† æ³¨æ„è¿™é‡Œ
```

**å¤„ç†å™¨è¶…æ—¶**:
```
[zh_repair] TIMEOUT | request_id=... | elapsed_ms=30000 | fallback=PASS
```

**å¤„ç†å™¨é”™è¯¯**:
```
[zh_repair] ERROR | request_id=... | error=... | fallback=PASS
```

---

## ğŸ†˜ è·å–æ”¯æŒ

å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æ— æ³•è§£å†³é—®é¢˜ï¼š

1. **æ”¶é›†è¯Šæ–­ä¿¡æ¯**:
   - æœåŠ¡å¯åŠ¨å®Œæ•´æ—¥å¿—
   - é”™è¯¯ä¿¡æ¯æˆªå›¾
   - ç³»ç»Ÿç¯å¢ƒä¿¡æ¯ï¼ˆOSã€CUDA ç‰ˆæœ¬ã€Python ç‰ˆæœ¬ï¼‰

2. **æŸ¥çœ‹ç›¸å…³æ–‡æ¡£**:
   - [ç»´æŠ¤æŒ‡å—](./MAINTENANCE_GUIDE.md)
   - [æ¶æ„è®¾è®¡](./ARCHITECTURE.md)
   - [API å‚è€ƒ](./API_REFERENCE.md)

3. **è”ç³»å¼€å‘å›¢é˜Ÿ**

---

## ğŸ“‹ é—®é¢˜æŠ¥å‘Šæ¨¡æ¿

```markdown
### é—®é¢˜æè¿°
[ç®€çŸ­æè¿°é—®é¢˜]

### ç¯å¢ƒä¿¡æ¯
- OS: Windows 11 / Linux / macOS
- Python: 3.x.x
- CUDA: 12.1
- GPU: NVIDIA RTX 4060

### å¤ç°æ­¥éª¤
1. [æ­¥éª¤1]
2. [æ­¥éª¤2]
3. [æ­¥éª¤3]

### æœŸæœ›è¡Œä¸º
[æœŸæœ›çœ‹åˆ°ä»€ä¹ˆ]

### å®é™…è¡Œä¸º
[å®é™…çœ‹åˆ°ä»€ä¹ˆ]

### æ—¥å¿—è¾“å‡º
```
[ç²˜è´´ç›¸å…³æ—¥å¿—]
```

### å·²å°è¯•çš„è§£å†³æ–¹æ¡ˆ
- [ ] é‡å¯æœåŠ¡
- [ ] æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
- [ ] æŸ¥çœ‹æ–‡æ¡£
```

---

**æ›´æ–°**: 2026-01-19  
**ç»´æŠ¤**: å¼€å‘å›¢é˜Ÿ
