# -*- coding: utf-8 -*-
"""
Unified Semantic Repair Service
ç»Ÿä¸€è¯­ä¹‰ä¿®å¤æœåŠ¡ä¸»æ–‡ä»¶
"""

import sys
import io
import os
import time
import traceback
import signal
import logging
import uuid
from contextlib import asynccontextmanager
from typing import Dict

from fastapi import FastAPI
from pydantic import BaseModel
import torch
import gc

# å¼ºåˆ¶è®¾ç½®æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºä¸º UTF-8 ç¼–ç ï¼ˆWindows å…¼å®¹æ€§ï¼‰
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(
        sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=False
    )
    sys.stderr = io.TextIOWrapper(
        sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=False
    )

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [Unified SR] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å…¨å±€å¼‚å¸¸å¤„ç†ï¼ˆæ•è·æœªå¤„ç†çš„å¼‚å¸¸ï¼Œé˜²æ­¢æœåŠ¡å´©æºƒï¼‰
def handle_exception(exc_type, exc_value, exc_traceback):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    
    print("=" * 80, flush=True)
    print(f"[Unified SR] ğŸš¨ Uncaught exception in main process, service may crash", flush=True)
    print(f"[Unified SR] Exception type: {exc_type.__name__}", flush=True)
    print(f"[Unified SR] Exception value: {exc_value}", flush=True)
    print("[Unified SR] Traceback:", flush=True)
    for line in traceback.format_exception(exc_type, exc_value, exc_traceback):
        print(f"[Unified SR] {line.rstrip()}", flush=True)
    print("=" * 80, flush=True)
    
    # è°ƒç”¨é»˜è®¤å¼‚å¸¸å¤„ç†å™¨
    sys.__excepthook__(exc_type, exc_value, exc_traceback)

sys.excepthook = handle_exception

# ä¿¡å·å¤„ç†ï¼ˆç”¨äºè®°å½•ä¸»è¿›ç¨‹é€€å‡ºï¼‰
def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    print(f"[Unified SR] Received signal {signum}, preparing to shutdown...", flush=True)
    if signum == signal.SIGTERM:
        print("[Unified SR] SIGTERM received, graceful shutdown", flush=True)
    elif signum == signal.SIGINT:
        print("[Unified SR] SIGINT received (Ctrl+C), graceful shutdown", flush=True)
    else:
        print(f"[Unified SR] Unexpected signal {signum} received", flush=True)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆWindows ä¸Šå¯èƒ½ä¸æ”¯æŒæ‰€æœ‰ä¿¡å·ï¼‰
try:
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
except (ValueError, OSError) as e:
    # Windows ä¸Šå¯èƒ½ä¸æ”¯æŒæŸäº›ä¿¡å·
    print(f"[Unified SR] Warning: Could not register signal handler: {e}", flush=True)


def log_resource_usage(stage: str, device=None):
    """è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ"""
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        cpu_percent = process.cpu_percent(interval=0.1)
        
        msg = f"[Unified SR] Resource Usage [{stage}]: Memory={memory_mb:.1f}MB, CPU={cpu_percent:.1f}%"
        
        if device and torch.cuda.is_available():
            try:
                gpu_mem_allocated = torch.cuda.memory_allocated(device) / 1024 / 1024 / 1024
                gpu_mem_reserved = torch.cuda.memory_reserved(device) / 1024 / 1024 / 1024
                msg += f", GPU_Allocated={gpu_mem_allocated:.2f}GB, GPU_Reserved={gpu_mem_reserved:.2f}GB"
            except:
                pass
        
        print(msg, flush=True)
        logger.info(msg)
    except Exception as e:
        print(f"[Unified SR] Warning: Could not log resource usage: {e}", flush=True)

from config import Config
from base.models import RepairRequest, RepairResponse, HealthResponse
from base.processor_wrapper import ProcessorWrapper
from processors.base_processor import BaseProcessor
from processors.zh_repair_processor import ZhRepairProcessor
from processors.en_repair_processor import EnRepairProcessor
from processors.en_normalize_processor import EnNormalizeProcessor

# å…¨å±€å˜é‡
processors: Dict[str, BaseProcessor] = {}
processor_wrapper: ProcessorWrapper = None
config: Config = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global processors, processor_wrapper, config
    
    print("=" * 80, flush=True)
    print("[Unified SR] ===== Starting Unified Semantic Repair Service =====", flush=True)
    print("=" * 80, flush=True)
    
    # è®°å½•å¯åŠ¨å‰èµ„æº
    log_resource_usage("BEFORE_INIT")
    
    # å¯åŠ¨ï¼šåˆå§‹åŒ–æ‰€æœ‰å¤„ç†å™¨
    try:
        config = Config()
        
        print(f"[Unified SR] Configuration loaded:", flush=True)
        print(f"[Unified SR]   Host: {config.host}", flush=True)
        print(f"[Unified SR]   Port: {config.port}", flush=True)
        print(f"[Unified SR]   Timeout: {config.timeout}s", flush=True)
        print(f"[Unified SR]   Enabled processors:", flush=True)
        
        enabled = config.get_enabled_processors()
        
        # åˆå§‹åŒ–ä¸­æ–‡è¯­ä¹‰ä¿®å¤å¤„ç†å™¨
        if 'zh_repair' in enabled:
            print(f"[Unified SR]     - zh_repair (Chinese Semantic Repair)", flush=True)
            zh_processor = ZhRepairProcessor(enabled['zh_repair'])
            processors['zh_repair'] = zh_processor
            log_resource_usage("AFTER_ZH_INIT")
        
        # åˆå§‹åŒ–è‹±æ–‡è¯­ä¹‰ä¿®å¤å¤„ç†å™¨
        if 'en_repair' in enabled:
            print(f"[Unified SR]     - en_repair (English Semantic Repair)", flush=True)
            en_processor = EnRepairProcessor(enabled['en_repair'])
            processors['en_repair'] = en_processor
            log_resource_usage("AFTER_EN_INIT")
        
        # åˆå§‹åŒ–è‹±æ–‡æ ‡å‡†åŒ–å¤„ç†å™¨
        if 'en_normalize' in enabled:
            print(f"[Unified SR]     - en_normalize (English Normalize)", flush=True)
            norm_processor = EnNormalizeProcessor(enabled['en_normalize'])
            processors['en_normalize'] = norm_processor
            log_resource_usage("AFTER_NORM_INIT")
        
        # åˆ›å»ºå¤„ç†å™¨åŒ…è£…å™¨
        processor_wrapper = ProcessorWrapper(processors, timeout=config.timeout)
        
        print(f"[Unified SR] Service ready with {len(processors)} processor(s)", flush=True)
        log_resource_usage("SERVICE_READY")
        print("=" * 80, flush=True)
    
    except Exception as e:
        print(f"[Unified SR] [CRITICAL ERROR] Failed to initialize: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise
    
    yield  # åº”ç”¨è¿è¡ŒæœŸé—´
    
    # å…³é—­ï¼šæ¸…ç†æ‰€æœ‰å¤„ç†å™¨
    print("[Unified SR] ===== Shutting down Unified Semantic Repair Service =====", flush=True)
    log_resource_usage("BEFORE_SHUTDOWN")
    
    for name, processor in processors.items():
        try:
            await processor.shutdown()
            print(f"[Unified SR] âœ… {name} shut down", flush=True)
        except Exception as e:
            print(f"[Unified SR] âŒ Error shutting down {name}: {e}", flush=True)
    
    processors.clear()
    
    # æ¸…ç† GPU å†…å­˜
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            gc.collect()
            print("[Unified SR] âœ… GPU memory cache cleared", flush=True)
        except Exception as e:
            print(f"[Unified SR] âš ï¸  Could not clear GPU cache: {e}", flush=True)
    
    log_resource_usage("AFTER_SHUTDOWN")
    print("[Unified SR] âœ… Graceful shutdown completed", flush=True)


# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Unified Semantic Repair Service",
    version="1.0.0",
    lifespan=lifespan
)


# ==================== è·¯å¾„éš”ç¦»çš„ç«¯ç‚¹ï¼ˆé›¶ if-elseï¼‰ ====================

@app.post("/zh/repair", response_model=RepairResponse)
async def zh_repair(request: RepairRequest):
    """ä¸­æ–‡è¯­ä¹‰ä¿®å¤"""
    return await processor_wrapper.handle_request("zh_repair", request)


@app.post("/en/repair", response_model=RepairResponse)
async def en_repair(request: RepairRequest):
    """è‹±æ–‡è¯­ä¹‰ä¿®å¤"""
    return await processor_wrapper.handle_request("en_repair", request)


@app.post("/en/normalize", response_model=RepairResponse)
async def en_normalize(request: RepairRequest):
    """è‹±æ–‡æ ‡å‡†åŒ–"""
    return await processor_wrapper.handle_request("en_normalize", request)


# ==================== å…¼å®¹æ—§ASRæ¨¡å—çš„ç»Ÿä¸€ç«¯ç‚¹ ====================

@app.post("/repair", response_model=RepairResponse)
async def repair_unified(request: RepairRequest):
    """
    ç»Ÿä¸€ä¿®å¤ç«¯ç‚¹ï¼ˆå‘åå…¼å®¹ï¼‰
    
    æ ¹æ®è¯·æ±‚ä¸­çš„ lang å‚æ•°è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†å™¨ï¼š
    - lang='zh' â†’ ZhRepairProcessor
    - lang='en' â†’ EnRepairProcessor
    
    è¿™ä¸ªç«¯ç‚¹æ˜¯ä¸ºäº†å…¼å®¹æ—§çš„ASRæ¨¡å—è°ƒç”¨æ–¹å¼ã€‚
    æ–°çš„è°ƒç”¨åº”è¯¥ä½¿ç”¨è·¯å¾„éš”ç¦»çš„ç«¯ç‚¹ï¼š/zh/repair, /en/repair, /en/normalize
    """
    # æ ¹æ® lang å‚æ•°é€‰æ‹©å¤„ç†å™¨
    lang = request.lang if hasattr(request, 'lang') and request.lang else 'en'
    
    if lang == 'zh':
        return await processor_wrapper.handle_request("zh_repair", request)
    elif lang == 'en':
        return await processor_wrapper.handle_request("en_repair", request)
    else:
        # ä¸æ”¯æŒçš„è¯­è¨€ï¼Œè¿”å›PASS
        return RepairResponse(
            request_id=request.job_id or str(uuid.uuid4()),
            decision="PASS",
            text_out=request.text_in,
            confidence=1.0,
            diff=[],
            reason_codes=["UNSUPPORTED_LANGUAGE"],
            process_time_ms=0,
            processor_name="none"
        )


# ==================== å¥åº·æ£€æŸ¥ç«¯ç‚¹ ====================

class GlobalHealthResponse(BaseModel):
    """å…¨å±€å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    processors: Dict[str, HealthResponse]


@app.get("/health", response_model=GlobalHealthResponse)
async def global_health():
    """å…¨å±€å¥åº·æ£€æŸ¥"""
    health_status = {}
    overall_healthy = True
    
    for name, processor in processors.items():
        try:
            status = await processor.get_health()
            health_status[name] = status
            if status.status != 'healthy':
                overall_healthy = False
        except Exception as e:
            logger.error(f"Error checking health for {name}: {e}")
            health_status[name] = HealthResponse(
                status='error',
                processor_type='unknown',
                initialized=False
            )
            overall_healthy = False
    
    return GlobalHealthResponse(
        status='healthy' if overall_healthy else 'degraded',
        processors=health_status
    )


@app.get("/zh/health", response_model=HealthResponse)
async def zh_health():
    """ä¸­æ–‡å¤„ç†å™¨å¥åº·æ£€æŸ¥"""
    processor = processors.get('zh_repair')
    if not processor:
        return HealthResponse(
            status='unavailable',
            processor_type='model',
            initialized=False
        )
    return await processor.get_health()


@app.get("/en/health", response_model=HealthResponse)
async def en_health():
    """è‹±æ–‡å¤„ç†å™¨å¥åº·æ£€æŸ¥ï¼ˆrepair + normalizeï¼‰"""
    # æ£€æŸ¥ä»»ä¸€è‹±æ–‡å¤„ç†å™¨çš„çŠ¶æ€
    repair_processor = processors.get('en_repair')
    norm_processor = processors.get('en_normalize')
    
    if repair_processor:
        return await repair_processor.get_health()
    elif norm_processor:
        return await norm_processor.get_health()
    else:
        return HealthResponse(
            status='unavailable',
            processor_type='unknown',
            initialized=False
        )


# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    import uvicorn
    
    # åŠ è½½é…ç½®
    cfg = Config()
    
    print(f"[Unified SR] Starting server on {cfg.host}:{cfg.port}", flush=True)
    print(f"[Unified SR] Python version: {sys.version}", flush=True)
    print(f"[Unified SR] PyTorch version: {torch.__version__}", flush=True)
    print(f"[Unified SR] CUDA available: {torch.cuda.is_available()}", flush=True)
    if torch.cuda.is_available():
        print(f"[Unified SR] CUDA device: {torch.cuda.get_device_name(0)}", flush=True)
    print("=" * 80, flush=True)
    
    uvicorn.run(
        app,
        host=cfg.host,
        port=cfg.port,
        log_level="info",
        workers=1,  # å•è¿›ç¨‹
        loop="asyncio"
    )
