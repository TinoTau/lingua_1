# 同音纠错 GPU 潜力与语义修复超时说明

## 1. 同音纠错若用 GPU，效率能提高多少？

### 当前实现（CPU）

- **技术栈**：KenLM（字符级 3-gram，C++ 实现 + Python 绑定）+ 同音混淆集。
- **流程**：繁→简 → 找可替换位点 → 生成候选句 → 对原文和候选逐句调用 `scorer.score()`（KenLM）→ 按 delta 选优。
- **典型耗时**：单句几十毫秒到一两百毫秒（与句长、候选数有关），已在 CPU 上非常快。

### 若改为 GPU

- **KenLM 本身没有官方 GPU 版**，若要“同音纠错用 GPU”，需要**换成神经语言模型**（如小 BERT、LSTM 等）在 GPU 上推理。
- **单句延迟**：
  - GPU 有 kernel 启动、数据传输等固定开销，对**单条短句**（十几到几十字）往往**提升有限**，甚至可能略慢。
  - 若同音纠错服务**批量处理多句**，GPU 批推理才有明显优势（常见 2–5 倍吞吐提升）。
- **结论**：  
  - **单句实时场景**（当前用法）：同音纠错用 GPU **收益有限**，KenLM 在 CPU 上已经足够快，瓶颈多在 ASR/语义修复/NMT。  
  - **批量离线场景**：若改为 GPU 神经 LM 并做批处理，**吞吐可提升约 2–5 倍**，单句延迟不一定更好。

---

## 2. 语义修复为什么会超时？是句子太长还是 GPU 仲裁超时太短？

### 超时来源：**不是** GPU 仲裁，是**节点 → 语义修复服务的 HTTP 超时**

- 语义修复在节点端**不经过 GPU 仲裁器**：节点只向独立的语义修复服务（如 5015）发 **HTTP POST**，由该服务内部用 GPU（LlamaCpp/LLM）做推理。
- 超时发生在**节点侧**：对这次 HTTP 请求设置了**请求超时**（之前为 **10 秒**）。若语义修复服务在 10 秒内没有返回，节点会报 `SEM_REPAIR_TIMEOUT: SERVICE_TIMEOUT` 并中止。

### 为何会超过 10 秒？

- **句子较长**：输入文本长（如 58 字、116 字），LLM 生成时间随长度和复杂度上升，容易超过 10 秒。
- **模型与硬件**：模型较大、GPU 忙或负载高时，单次推理时间变长。
- **10 秒偏短**：对长句或负载高时，10 秒的阈值本身偏紧，容易触发超时。

### 已做修改

- 已将节点侧**语义修复 HTTP 超时**从 **10 秒** 调整为 **30 秒**（`task-router-semantic-repair.ts` 中 `callSemanticRepairService`）。
- 若仍偶发超时，可考虑：
  - 再适当提高该超时（如 60 秒），或
  - 在语义修复服务端做优化：更小模型、限制生成长度、或排队/限流避免单次推理过久。

### 小结

| 问题 | 答案 |
|------|------|
| 超时是“句子太长”还是“GPU 仲裁超时太短”？ | **都不是**。是**节点对语义修复服务的 HTTP 超时**（原先 10 秒）太短；长句或慢推理时服务未在 10 秒内返回。 |
| GPU 仲裁和语义修复超时有关吗？ | **无关**。语义修复不占节点 GPU 租约，超时与 GPU 仲裁配置无直接关系。 |
